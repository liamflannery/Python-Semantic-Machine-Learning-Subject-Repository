{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\liamf\\OneDrive\\Desktop\\Uni\\Document_Processing\\2022S1\\assignments\\assignment-3-start\n",
    "# python -m http.server 8080\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "html = requests.get(url = 'http://localhost:8080/student.html').text\n",
    "htmlParse = BeautifulSoup(html, 'html.parser').find_all('p')\n",
    "\n",
    "text = ' '.join(x.string for x in htmlParse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  subj = \"\"           # Variable for storing the subject.\n",
    "  obj = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # Variable for dependency tag of previous token in the sentence. \n",
    "  prv_tok_text = \"\"   # Variable for previous token in the sentence.\n",
    "\n",
    "  prefix = \"\"         # Variable for storing compounds.\n",
    "  modifier = \"\"       # Variable for storing modifieres.\n",
    "\n",
    "  # Loop through the tokens in the sentence.   \n",
    "  for idt, tok in enumerate(sent):\n",
    "    # Check if a token is a punctuation mark or not.\n",
    "    if tok.dep_ != \"punct\":\n",
    "      if tok.dep_ == 'attr':\n",
    "          if(idt == len(sent) - 1):\n",
    "                prefix = prv_tok_text + \" \" + tok.text\n",
    "      # Check if a token is a compound one or not.\n",
    "      if tok.dep_ == \"compound\":\n",
    "        # If yes, then store the token in the prefix variable.\n",
    "        prefix = tok.text\n",
    "        # Check if the previous token was also a compound one.\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          # If yes, then update the prefix variable.\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # Check if a token is a modifier or not.\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        # If yes, then store the token in the modifier varible.\n",
    "        modifier = tok.text\n",
    "         # Check if the previous token was a compound one.\n",
    "        if prv_tok_dep == \"compound\":\n",
    "        # If yes, then update the modifier variable.  \n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "    \n",
    "      # Check if a token is the subject.\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        # If yes, then concatenate the modifier, prefix, and token\n",
    "        # and assign the result to the subject variable (ent1).\n",
    "        subj = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        # Reset the following variables: prefix, modifier, prv_tok_dep, and prv_tok_text.\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      # Check if a token is the object.\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        # If yes, then concatenate the modifier, prefix, and token \n",
    "        # and assign the result to the object variable (ent2).    \n",
    "        obj = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      # Update the variable for the dependency tag for the previous token. \n",
    "      prv_tok_dep = tok.dep_\n",
    "      # Update the variable for the previous token in the sentence.  \n",
    "      prv_tok_text = tok.text\n",
    "\n",
    "\n",
    " \n",
    "  return [subj.strip(), obj.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpred(sent):\n",
    "    sentance = []\n",
    "    for x in sent:\n",
    "        if(x.dep_ != 'punct'):\n",
    "            sentance.append(x)\n",
    "       \n",
    "    if(sentance[len(sentance) -1].dep_ == \"attr\"):\n",
    "        sentance = sentance[:len(sentance) - 1]\n",
    "    for idx, token in enumerate(sentance):\n",
    "       \n",
    "        if(idx < len(sentance) - 1):\n",
    "            \n",
    "            if(token.dep_ == 'det'):\n",
    "                if(sentance[idx+1].dep_ == 'attr' or sentance[idx+1].dep_ == 'compound'):\n",
    "                    sentance.remove(token)\n",
    "                    continue\n",
    "        if(token.dep_ == 'prep'):\n",
    "            sentance = sentance[:idx + 1]\n",
    "            break\n",
    "            \n",
    "    \n",
    "    should_restart = True\n",
    "    while(should_restart):\n",
    "        for token in sentance:\n",
    "            if(token.dep_ == 'aux' or token.dep_ == 'auxpass' or token.dep_ == 'ROOT'):\n",
    "                should_restart = False\n",
    "                break\n",
    "            else:\n",
    "                sentance.remove(token)\n",
    "                should_restart = True\n",
    "                break\n",
    "\n",
    "    if(sentance[len(sentance) -1].dep_ == \"nummod\"):\n",
    "        sentance = sentance[:len(sentance) - 1]\n",
    "    return sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kevin Walker is a student., Kevin Walker was born on 2001/07/24., Kevin Walker lives in Epping., Kevin Walker works at ALDI., Kevin is a friend of Alice Miller., Kevin is studying at Macquarie University., He has the student number 40048822., He is enrolled in COMP3100, He is enrolled in COMP3220 , Alice Miller is an alumna of Macquarie University., She is a friend of Kevin Walker.]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "model = nlp(text)\n",
    "sentances = []\n",
    "\n",
    "\n",
    "for s in model.sents:\n",
    "    sentances.append(s)\n",
    "\n",
    "## splitting up sentnances with a 'cc' or joining word \n",
    "for sidx, sentance in enumerate(sentances):\n",
    "    for idx, token in enumerate(sentance):\n",
    "        if(token.dep_ == 'cc'):\n",
    "            newsentance1 = sentance[idx:]\n",
    "            newsentance2 = sentance[:idx]\n",
    "\n",
    "            for newtoken in newsentance1:\n",
    "                if(newtoken.dep_ == 'pobj'):\n",
    "                    storetoken = newtoken\n",
    "            \n",
    "            newsentance1 = newsentance2\n",
    "\n",
    "            newstring = \"\"\n",
    "            for ridx, replacetoken in enumerate(newsentance1):\n",
    "                if(replacetoken.dep_ == 'pobj'):\n",
    "                    newstring += storetoken.text\n",
    "                else:\n",
    "                    newstring += replacetoken.text\n",
    "                newstring += \" \"\n",
    "         \n",
    "            newsentance3 = nlp(newstring)\n",
    "            sentances.remove(sentance)\n",
    "            sentances.insert(sidx, newsentance3)\n",
    "            sentances.insert(sidx, newsentance1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kevin Walker', '']\n",
      "['Kevin Walker', '2001/07/24']\n",
      "['Kevin Walker', 'Epping']\n",
      "['Kevin Walker', 'ALDI']\n",
      "['Kevin', 'Alice Miller']\n",
      "['Kevin', 'Macquarie University']\n",
      "['He', 'student number']\n",
      "['He', 'COMP3100']\n",
      "['He', 'COMP3220']\n",
      "['Alice Miller', 'Macquarie University']\n",
      "['She', 'Kevin Walker']\n"
     ]
    }
   ],
   "source": [
    "for current in sentances:\n",
    "    print(get_entities(current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kevin Walker', '']\n",
      "Kevin -> compound\n",
      "Walker -> nsubj\n",
      "is -> ROOT\n",
      "a -> det\n",
      "student -> attr\n",
      ". -> punct\n"
     ]
    }
   ],
   "source": [
    "current = sentances[0]\n",
    "print(get_entities(current))\n",
    "for x in current:\n",
    "         print(x,'->', x.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "source = [\"Kevin Walker\", \"Kevin Walker\",\"Kevin Walker\",\"Kevin Walker\",\"Kevin Walker\",\"Kevin Walker\",\"Kevin Walker\",\"Kevin Walker\",\"Kevin Walker\",\"Alice Miller\", \"Alice Miller\"]\n",
    "edge = [\"is a\", \"born on\", \"lives in\", \"works at\", \"is friend of\", \"is studying at\", \"has student number\", \"is enrolled in1\", \"is enrolled in2\", \"is alumna of\", \"is friend of\"]\n",
    "target = [\"student\", \"2001-07-24\", \"Epping\", \"ALDI\", \"Alice Miller\", \"Macquarie University\", \"40048822\", \"COMP3100\", \"COMP3220\", \"Macquarie University\", \"Kevin Walker\"]\n",
    "d = {'name': source, 'edge': edge, 'target' : target}\n",
    "# combine = zip(edge, target)\n",
    "# d = dict(combine)\n",
    "data = pd.DataFrame(data = d)\n",
    "#data.to_csv(\"schemas.csv\")\n",
    "# peopleArray = data['source'].unique()\n",
    "# peopleData = {'name' : peopleArray}\n",
    "# peopleDF = pd.DataFrame(peopleData)\n",
    "# peopleDF.to_csv(\"people.csv\")\n",
    "\n",
    "\n",
    "\n",
    "data = pd.DataFrame(columns = edge)\n",
    "\n",
    "\n",
    "\n",
    "for idx, element in enumerate(edge):\n",
    "    data.at[source[idx], edge[idx]] = target[idx] \n",
    "\n",
    "\n",
    "data.index.name = 'name'\n",
    "data\n",
    "data.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"python -m rdfizer -c ./config.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20496a84efe115372f44d1b35d571fff313331298d2cdbaebef32413764fe580"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
