{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\liamf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\liamf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\liamf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31998"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "nltk.download('universal_tagset')\n",
    "def count_pos(document, pos):\n",
    "    \"\"\"Return the number of occurrences of words with a given part of speech. To find the part of speech, use \n",
    "    NLTK's \"Universal\" tag set. To find the words of the document, use NLTK's sent_tokenize and word_tokenize.\n",
    "    >>> count_pos('austen-emma.txt', 'NOUN')\n",
    "    31998\n",
    "    >>> count_pos('austen-sense.txt', 'VERB')\n",
    "    25074\n",
    "    \"\"\"\n",
    "    t = []\n",
    "    for s in nltk.sent_tokenize(nltk.corpus.gutenberg.raw(document)):\n",
    "        i = nltk.word_tokenize(s)\n",
    "        t += nltk.pos_tag(i, tagset='universal')\n",
    "\n",
    "    \n",
    "    d = [w for [n,w] in t]\n",
    "    counter = collections.Counter(d)\n",
    "    return counter[pos]\n",
    "\n",
    "count_pos('austen-emma.txt', 'NOUN')  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'and'), ('.', \"''\"), (';', 'and'), ('to', 'be')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_stem_bigrams(document, n):\n",
    "    \"\"\"Return the n most frequent bigrams of stems. Return the list sorted in descending order of frequency.\n",
    "    The stems of words in different sentences cannot form a bigram. To stem a word, use NLTK's Porter stemmer.\n",
    "    To find the words of the document, use NLTK's sent_tokenize and word_tokenize.\n",
    "    >>> get_top_stem_bigrams('austen-emma.txt', 3)\n",
    "    [(',', 'and'), ('.', \"''\"), (';', 'and')]\n",
    "    >>> get_top_stem_bigrams('austen-sense.txt',4)\n",
    "    [(',', 'and'), ('.', \"''\"), (';', 'and'), (',', \"''\")]\n",
    "    \"\"\"\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    words = []\n",
    "    bigrams = list()\n",
    "    for sentance in nltk.sent_tokenize(nltk.corpus.gutenberg.raw(document)):\n",
    "        words = [stemmer.stem(word) for word in nltk.word_tokenize(sentance)]\n",
    "        bigrams += list(nltk.bigrams(words))\n",
    "    \n",
    "    counter = collections.Counter(bigrams).most_common(n)\n",
    "    counter_small = [x for [x,y] in counter]\n",
    "    return counter_small\n",
    "get_top_stem_bigrams('austen-emma.txt', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Respect', 2),\n",
       " ('respect', 41),\n",
       " ('respectability', 1),\n",
       " ('respectable', 20),\n",
       " ('respectably', 1),\n",
       " ('respected', 2),\n",
       " ('respectful', 2),\n",
       " ('respectfully', 1),\n",
       " ('respecting', 2),\n",
       " ('respective', 1),\n",
       " ('respects', 6)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_same_stem(document, word):\n",
    "    \"\"\"Return the list of words that have the same stem as the word given, and their frequencies. \n",
    "    To find the stem, use NLTK's Porter stemmer. To find the words of the document, use NLTK's \n",
    "    sent_tokenize and word_tokenize. The resulting list must be sorted alphabetically.\n",
    "    >>> get_same_stem('austen-emma.txt','respect')[:5]\n",
    "    [('Respect', 2), ('respect', 41), ('respectability', 1), ('respectable', 20), ('respectably', 1)]\n",
    "    >>> get_same_stem('austen-sense.txt','respect')[:5]\n",
    "    [('respect', 22), ('respectability', 1), ('respectable', 14), ('respectably', 1), ('respected', 3)]\n",
    "    \"\"\"\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    words = dict()\n",
    "    targetStem = stemmer.stem(word)\n",
    "    for sentance in nltk.sent_tokenize(nltk.corpus.gutenberg.raw(document)):\n",
    "        for currentWord in nltk.word_tokenize(sentance):\n",
    "            if stemmer.stem(currentWord) == targetStem:\n",
    "                if(currentWord in words.keys()):\n",
    "                    words[currentWord] += 1\n",
    "                else:\n",
    "                    words[currentWord] = 1\n",
    "    listWords = words.items()\n",
    "    sortedWords = sorted(listWords)\n",
    "    return sortedWords\n",
    "                    \n",
    "get_same_stem('austen-emma.txt','respect')            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', 1932)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_frequent_after_pos(document, pos):\n",
    "    \"\"\"Return the most frequent word after a given part of speech, and its frequency. Do not consider words\n",
    "    that occur in the next sentence after the given part of speech.\n",
    "    To find the part of speech, use NLTK's \"Universal\" tagset.\n",
    "    >>> most_frequent_after_pos('austen-emma.txt','VERB')\n",
    "    [('not', 1932)]\n",
    "    >>> most_frequent_after_pos('austen-sense.txt','NOUN')\n",
    "    [(',', 5310)]\n",
    "    \"\"\"\n",
    "    followWords = dict()\n",
    "    prevWord = False;\n",
    "    for sentance in nltk.sent_tokenize(nltk.corpus.gutenberg.raw(document)):\n",
    "         currentWord = nltk.word_tokenize(sentance)\n",
    "         for tokenWord in nltk.pos_tag(currentWord, tagset='universal'):\n",
    "             currentPart = tokenWord[1]\n",
    "             \n",
    "             if(prevWord):\n",
    "                 if(tokenWord[0] in followWords.keys()):\n",
    "                    followWords[tokenWord[0]] += 1\n",
    "                 else:\n",
    "                    followWords[tokenWord[0]] = 1\n",
    "             prevWord = False;\n",
    "             if(currentPart == pos):\n",
    "                 prevWord = True;         \n",
    "              \n",
    "    followWords = sorted(followWords.items(), key = lambda item: item[1], reverse = True)\n",
    "    return [followWords[0]]\n",
    "most_frequent_after_pos('austen-emma.txt','VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TfidfVectorizer' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-ed521a86d88f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mget_word_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Brutus is a honourable person'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-ed521a86d88f>\u001b[0m in \u001b[0;36mget_word_tfidf\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidfVectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidfVectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TF-IDF\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TF-IDF'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TfidfVectorizer' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def get_word_tfidf(text):\n",
    "    \"\"\"Return the tf.idf of the words given in the text. If a word does not have tf.idf information or is zero, \n",
    "    then do not return its tf.idf. The reference for computing tf.idf is the list of documents from the NLTK \n",
    "    Gutenberg corpus. To compute the tf.idf, use sklearn's TfidfVectorizer with the option to remove the English \n",
    "    stop words (stop_words='english'). The result must be a list of words sorted in alphabetical order, together \n",
    "    with their tf.idf.\n",
    "    >>> get_word_tfidf('Emma is a respectable person')\n",
    "    [('emma', 0.8310852062844262), ('person', 0.3245184217533661), ('respectable', 0.4516471784898886)]\n",
    "    >>> get_word_tfidf('Brutus is a honourable person')\n",
    "    [('brutus', 0.8405129362379974), ('honourable', 0.4310718596448824), ('person', 0.32819971943754456)]\n",
    "    \"\"\"\n",
    "    tfidfVectorizer = TfidfVectorizer(input='content',stop_words='english')\n",
    "    data = [nltk.corpus.gutenberg.raw(f) for f in nltk.corpus.gutenberg.fileids()]\n",
    "    tfidf = tfidfVectorizer.fit_transform(data)\n",
    "    df = pd.DataFrame(tfidf[0].T.todense(), index=tfidfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    result = []\n",
    "    df = df[\"TF-IDF\"].to_dict()\n",
    "   ## print(df)\n",
    "   ## print(df[\"TF-IDF\"][\"emma\"].values())\n",
    "    dfItems = df.items()\n",
    "   \n",
    "    for word in text.split(\" \"):\n",
    "         for i in dfItems:\n",
    "            if(i[0] == word.lower() and i[1] > 0):\n",
    "                result += i\n",
    "    return result\n",
    "get_word_tfidf('Brutus is a honourable person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(optionflags=doctest.ELLIPSIS)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d4eb222e854d6dcf0ad3595ad06f3f990a60f1352b3f283c2ce0fb8668d5aef"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
