{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh"
   },
   "source": [
    "# Assignment 2 - Find complex answers to medical questions\n",
    "\n",
    "**Submission deadline: Friday 22 April, 5pm.** \n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**.\n",
    "\n",
    "**Assessment marks: 20 marks (20% of the total unit assessment)**\n",
    "\n",
    "In this assignment we will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question.\n",
    "\n",
    "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0).\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip data.zip\n",
    "!tar xzf data.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Hirschsprung disease (HSCR) is a multifactoria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>In this study, we review the identification of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The majority of the identified genes are relat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The non-Mendelian inheritance of sporadic non-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Coding sequence mutations in e.g.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid  sentid                                           question  \\\n",
       "0    0       0  Is Hirschsprung disease a mendelian or a multi...   \n",
       "1    0       1  Is Hirschsprung disease a mendelian or a multi...   \n",
       "2    0       2  Is Hirschsprung disease a mendelian or a multi...   \n",
       "3    0       3  Is Hirschsprung disease a mendelian or a multi...   \n",
       "4    0       4  Is Hirschsprung disease a mendelian or a multi...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  Hirschsprung disease (HSCR) is a multifactoria...      0  \n",
       "1  In this study, we review the identification of...      1  \n",
       "2  The majority of the identified genes are relat...      1  \n",
       "3  The non-Mendelian inheritance of sporadic non-...      1  \n",
       "4                  Coding sequence mutations in e.g.      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xqxCmR0dCk"
   },
   "source": [
    "# Task 1 (5 marks): Data preparation\n",
    "\n",
    "Partition the data into the training, dev_test, and test sets using the proportions 6:2:2. That is, 60% of the questions must be in the training set, 20% must be in the dev_test set, and the remaining 20% in the test set. Make sure that you partition based on the questions, not on the rows. With this we mean that all the sentences related to a question must be in one file only. In other words, there must not be sentences from the same question in, say, the training and the test data.\n",
    "\n",
    "Also, make sure that you implement a random partition.\n",
    "\n",
    "Save the partitions as the files `training.csv`, `dev_test.csv`, and `test.csv`, so that they can be used by other people.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if your explanation answers the following question correctly: Why do we want to split the partition on the questions, and not on the rows?\n",
    "* **1 mark** if the code partitions the data on the questions randomly and according to the split 6:2:2.\n",
    "* **1 mark** if your code generates partitions that have similar balance of labels and you demonstrate that they are similar.\n",
    "* **1 mark** if the partitions are saved as the CSV files `training.csv`, `dev_test.csv`, and `test.csv`.\n",
    "* **1 mark** for good coding and documentation in this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why questions, not rows?\n",
    "The data is split up based on each question rather than each row as we do not want the same question to be in the training, test or devtest set. This is because training and testing with the same data (or categories of data such as the same question) could easily lead to overfitting. For example, if we were to train a model using a question, and then were to test the model using the same question it would be able to answer it based on the data given to it rather than from learning relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input seed for random assortment of questions, will have same assortment for same seed (allows for reproducible results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to divide the dataset whilst keeping the questions together we must do the following:\n",
    "* Create another data set which contains one row for each question, along with each index in the original dataset that corresponds to the question\n",
    "* Randomly shuffle this new dataset\n",
    "* Divide the new dataset into three buckets representing train, dev test and test sets based on the given ratios (60/20/20)\n",
    "* Divide the original dataset into train, dev test and test sets by looking up which 'bucket' each question belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['dataset index'] = dataset.index #add new column to keep train of the original datasets index\n",
    "groupedDataSet = dataset.groupby('qid').agg(list) #create new dataset with one row for each question\n",
    "groupedDataSet = groupedDataSet.sample(frac = 1, random_state= seed) #randomly shuffle new dataset based on 'seed'\n",
    "size = len(groupedDataSet.index)\n",
    "# divide new data set into 'buckets'\n",
    "groupedTrain = groupedDataSet[:(int)(size * 0.6)]\n",
    "groupedDT = groupedDataSet[(int)(size * 0.6):(int)(size * 0.8)]\n",
    "groupedTest = groupedDataSet[(int)(size * 0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of which indices from the original data set belong to which bucket\n",
    "trainIndexList = groupedTrain['dataset index'].tolist()\n",
    "newIndexList = []\n",
    "for x in trainIndexList:\n",
    "    for y in x:\n",
    "        newIndexList.append(y)\n",
    "\n",
    "trainIndexList = newIndexList\n",
    "\n",
    "dTrainIndexList = groupedDT['dataset index'].tolist()\n",
    "newIndexList = []\n",
    "for x in dTrainIndexList:\n",
    "    for y in x:\n",
    "        newIndexList.append(y)\n",
    "\n",
    "dTrainIndexList = newIndexList\n",
    "\n",
    "testIndexList = groupedTest['dataset index'].tolist()\n",
    "newIndexList = []\n",
    "for x in testIndexList:\n",
    "    for y in x:\n",
    "        newIndexList.append(y)\n",
    "\n",
    "testIndexList = newIndexList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Su2UC0Ti0dCo",
    "outputId": "4a705b65-e2e5-4415-a53b-a97b79bc2eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 38657\n",
      "Training set ratio:  0.29634994955635463\n",
      "Size of devtest set: 12791\n",
      "Dev Test set ratio:  0.29817840669220547\n",
      "Size of test set: 12664\n",
      "Test set ratio:  0.3034586228679722\n"
     ]
    }
   ],
   "source": [
    "# divide assign indices from original dataset to each 'bucket'\n",
    "train_set = dataset.iloc[trainIndexList]\n",
    "devtest_set = dataset.iloc[dTrainIndexList]\n",
    "test_set = dataset.iloc[testIndexList]\n",
    "\n",
    "\n",
    "print(\"Size of training set:\", len(train_set))\n",
    "print(\"Training set ratio: \", train_set['label'].sum() / len(train_set))\n",
    "print(\"Size of devtest set:\", len(devtest_set))\n",
    "print(\"Dev Test set ratio: \", devtest_set['label'].sum() / len(devtest_set))\n",
    "print(\"Size of test set:\", len(test_set))\n",
    "print(\"Test set ratio: \", test_set['label'].sum() / len(test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the sizes of each set turn out to be roughly equivalent to a 60/20/20 split of the original data. \n",
    "I have also included an ouput of the ratio of each positive label to the total size of each data set (by adding up the total labels and dividing by the size of the dataframe). These ratios being similar would indicate there that there is an even split of labels between each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results into csv files\n",
    "train_set.to_csv('training.csv')\n",
    "devtest_set.to_csv('dev_test.csv')\n",
    "test_set.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbUJWlD_0dCv"
   },
   "source": [
    "# Task 2 (5 marks): Cosine similarity\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us in the file `data.zip` (so that any possible errors that you may have introduced in task 1 do not propagate to this task and following tasks).\n",
    "\n",
    "Implement a simple text summariser that is based on the cosine similarity between the question and the text. Use the following function signature.\n",
    "\n",
    "```{python}\n",
    "def cosine_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest cosine similarity\n",
    "    with the question. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[3, 1, 4], [12, 4, 13]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "To obtain the text vectors, use sklearn's tf.idf libraries this way:\n",
    "\n",
    "* Use all the defaults from the TfidfVectorizer instance, except for `stop_words=\"english\"` and `max_features=10000`. The latter option will restrict the vocabulary size to 10,000. This will speed up the computations and reduce the memory footprint in the subsequent tasks.\n",
    "* Use the `fit` method on the text of `training.csv`. In your documentation, please explain and justify what decision choices you made to select the correct text: would you use the question text only, the sentence text, or both?\n",
    "\n",
    "Evaluate the summariser by reporting the mean F1 score on each of the three CSV files `training.csv`, `devtest.csv`, and `test.csv`, for $n=5$. To calculate the mean F1 score, do this:\n",
    "\n",
    "1. For each question ID in the file, calculate the F1 score by comparing the result of your cosine summariser and the given labels. Feel free to use sklearn's functions to compute the F1 score, or implement your own version of the F1 scoring function if you prefer.\n",
    "2. Calculate the mean of the F1 scores calculated in step 1.\n",
    "\n",
    "Find the value of $n$ that returns the highest mean F1 score on the dev_test data.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the code generates the tf.idf vectors correctly. The explanations that justify the decisions made are reasonable. In particular, explain and justify what information you used to fit tf.idf.\n",
    "* **1 mark** if the code calculates cosine similarity correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest cosine similarity with the question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the dev_test file and identifies the value of $n$ that gives the highest score on the dev_test file.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, comment on the reason why you think the value of $n$ that gives highest F1 has that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements and establishing a training set to work with\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "training = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the TF.IDF Vectorizer\n",
    "The tf.idf vecotrizer is created and trained using both the sentence text and the question text. As we are trying to learn how well a certain sentence answers a certain question, it is important to tell our model what those questions are. If we were to train the model with just sentence text, it would not be able to learn why a particular sentence answers a question. A sentence may be a good answer for one question but a bad answer for another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=10000, stop_words='english')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the tfidf vecotizer \n",
    "vectorizer = TfidfVectorizer(use_idf=True, stop_words='english', max_features=10000)\n",
    "vectorizer.fit(training['sentence text'], training['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine summariser outputs the top n answers to each of the propsed question ids.\n",
    "This performs this operation with the following steps:\n",
    "<ol>\n",
    "    <li> Loops through each given question id. </li>\n",
    "    <li> Stores the tf.idf of the current question </li>\n",
    "    <li> Loops through each answer to the current question </li>\n",
    "    <li> Finds the tf.idf of the current answer to the current question </li>\n",
    "    <li> Finds the dot product of the answer tf.idf and the question tf.idf </li>\n",
    "    <li> Adds the dot product to the results list if it is better than at least one of the current elements </li>\n",
    "    <li> Returns a sorted list (by dot product score) of answer ids for the question </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_summariser(csvfile, questionids, n=5):\n",
    "      totalResults = []\n",
    "      totalResults_scores = []\n",
    "      file = pd.read_csv(csvfile)\n",
    "      \n",
    "      for qid in questionids: #loops through each question id\n",
    "         if(len(file[file.qid == qid]) > 0): #only performs the operation if the question id is in the dataframe\n",
    "            #finds tf.idf score of the current question\n",
    "            questionVector = vectorizer.transform([file[file.qid == qid]['question'].to_numpy()[0]]).toarray()[0]\n",
    "            results = []\n",
    "            results_scores = []\n",
    "            results_worst = 1\n",
    "            \n",
    "            df = file[file['qid'] == qid].copy()\n",
    "            #loops through each answer\n",
    "            for i in range(0,len(df.index)):\n",
    "               #finds the tf.idf of the current answer\n",
    "               answerVector = vectorizer.transform([df['sentence text'].iloc(0)[i]]).toarray()[0]\n",
    "               answerID = df['sentid'].iloc(0)[i]\n",
    "               #finds the dot product of the question and answer tf.idf scores\n",
    "               score = np.dot(questionVector, answerVector)\n",
    "               if len(results) < n:\n",
    "                  # It is one of the top n results by default\n",
    "                  results.append(answerID)\n",
    "                  results_scores.append(score)\n",
    "                  results_worst = np.min((results_worst, score))\n",
    "                  continue\n",
    "               if score > results_worst:\n",
    "                  # It is one of the top n results; replace with worst so far\n",
    "                  j = np.argmin(results_scores)\n",
    "                  results_scores[j] = score\n",
    "                  results[j] = answerID\n",
    "                  results_worst = np.min(results_scores)\n",
    "            results = sorted(results, key=lambda x: results_scores[results.index(x)], reverse=True)\n",
    "            totalResults.append(results)\n",
    "            totalResults_scores.append(results_scores)\n",
    "      \n",
    "      return sorted(totalResults)\n",
    "        \n",
    "       \n",
    "     #  \"\"\"Return the IDs of the n sentences that have the highest cosine similarity\n",
    "  #  with the question. The input questionids is a list of question ids. The \n",
    "  #  output is a list of lists of sentence ids\n",
    "   # >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    " #   [[3, 1, 4], [12, 4, 13]]\"\"\"\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Summariser \n",
    "We will evaluate the summariser following these steps:\n",
    "<ol>\n",
    "    <li> Loops through <i>n</i> values 1, 2, 3 ,4 and 5 </li>\n",
    "    <li> Loops through each question id in the devtest set </li>\n",
    "    <li> For the current dataframe, orders the sentences (answers) by their label and stores this array </li>\n",
    "    <li> Uses the cosine summariser to predict the order of the answers </li>\n",
    "    <li> Compares the actual values (from step 3) with the predicted values (from step 4) using an f1 summariser </li>\n",
    "    <li> Stores the value of the f1 summariser </li>\n",
    "    <li> Finds the average f1 score of all the questions </li>\n",
    "    <li> Stores each average f1 score for each value of <i> n </i> in an array </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "nValueScores = []\n",
    "for n in range(1,5):\n",
    "    f1Scores = []\n",
    "    for qid in devtest_set['qid'].unique():\n",
    "        actual = devtest_set[devtest_set.qid == qid].sort_values(by=['label'])['sentid'].to_numpy()[:n]\n",
    "        predicted = cosine_summariser('dev_test.csv', [qid], n)\n",
    "        predicted = np.array(predicted[0])\n",
    "        f1Scores.append(f1_score(actual, predicted, average='micro'))\n",
    "\n",
    "    nValueScores.append(np.mean(f1Scores))\n",
    "#print(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of n that gives the highest f1 score is:  1\n",
      "The accruacy was:  0.22195985832349469\n"
     ]
    }
   ],
   "source": [
    "nValue = nValueScores.index(max(nValueScores)) + 1\n",
    "print(\"The value of n that gives the highest f1 score is: \", nValue)\n",
    "print(\"The accruacy was: \", max(nValueScores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22195985832349469,\n",
       " 0.20306965761511217,\n",
       " 0.20621802439984258,\n",
       " 0.20297127115308933]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nValueScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My code returns that the value of <i> n </i> that gives the highest f1 score compared with my cosine summariser is 1.\n",
    "I believe that this is because the labels of the data are either 1 or 0. This means that if there are 5 sentences that answer a question, and 3 of them are labelled 1 the other 2 would be labelled 0. This means that ordering the sentence ids by their label would result in 3 equal first placed and 2 equal last placed elements. Because the cosine summariser has values other than 1 or 0, it can have 5 (or more) unique values and therefore will have more nuanced placements of its results. Essentially my code is comparing binary labels to multivalued prediction data. Because of this, as the value of n increases, there is more room for equally placed elements in the actual values to be in a different order to the predicted values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 3 (5 marks): Simple NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
    "\n",
    "1. An input layer that will accept the tf.idf of the sentence text (we will ignore the question text in this task). Use the TfidfVectorizer instance that you have fitted in task 2.\n",
    "2. A hidden layer and a relu activation function. You need to determine the size of the hidden layer.\n",
    "3. An output layer with one cell. The output layer will classify the input text (binary classification).\n",
    "\n",
    "Train the model with the training data and use the dev_test set to determine a good size of the hidden layer. \n",
    "\n",
    "With the model that you have trained, and implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
    "\n",
    "```{python}\n",
    "def nn_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest predicted score. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[2, 1, 3], [7, 14, 10]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the system developed in task 2. To make this task less time-consuming, focus only on $n=5$.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the tf.idf information of the text to the model correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models and Determining Hidden Layer Size\n",
    "We will create 3 seperate models with increasing hidden layer size (1, 5 and 7). We will then plot the accuracy of the validation and training scores over each epoch of the model. From these plots we will be able to see how the different hidden layer sizes will effect the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uY6sDbUn0dC6"
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#1 Hidden Layer\n",
    "model1HL = models.Sequential()\n",
    "model1HL.add(layers.Dense(1, activation='relu', input_shape = (len(vectorizer.get_feature_names()),)))\n",
    "model1HL.add(layers.Dense(1, activation='sigmoid'))\n",
    "model1HL.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#5 Hidden Layers\n",
    "model5HL = models.Sequential()\n",
    "model5HL.add(layers.Dense(5, activation='relu', input_shape = (len(vectorizer.get_feature_names()),)))\n",
    "model5HL.add(layers.Dense(1, activation='sigmoid'))\n",
    "model5HL.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#7 Hidden Layers\n",
    "model7HL = models.Sequential()\n",
    "model7HL.add(layers.Dense(7, activation='relu', input_shape = (len(vectorizer.get_feature_names()),)))\n",
    "model7HL.add(layers.Dense(1, activation='sigmoid'))\n",
    "model7HL.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the training and devtest data and labels\n",
    "train_data = vectorizer.transform(training['sentence text']).toarray()\n",
    "train_labels = training['label']\n",
    "devtest = pd.read_csv(\"dev_test.csv\")\n",
    "devtest_data = vectorizer.transform(devtest['sentence text']).toarray()\n",
    "devtest_labels = devtest['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#function to plot the training and validation accuracy of a model\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "387/387 [==============================] - 4s 9ms/step - loss: 0.6256 - accuracy: 0.7037 - val_loss: 0.6068 - val_accuracy: 0.7018\n",
      "Epoch 2/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5948 - accuracy: 0.7037 - val_loss: 0.6039 - val_accuracy: 0.7018\n",
      "Epoch 3/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5842 - accuracy: 0.7037 - val_loss: 0.6019 - val_accuracy: 0.7018\n",
      "Epoch 4/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5731 - accuracy: 0.7037 - val_loss: 0.6010 - val_accuracy: 0.7018\n",
      "Epoch 5/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5621 - accuracy: 0.7037 - val_loss: 0.6015 - val_accuracy: 0.7018\n",
      "Epoch 6/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5522 - accuracy: 0.7046 - val_loss: 0.6033 - val_accuracy: 0.7012\n",
      "Epoch 7/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5434 - accuracy: 0.7189 - val_loss: 0.6067 - val_accuracy: 0.6887\n",
      "Epoch 8/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5360 - accuracy: 0.7201 - val_loss: 0.6107 - val_accuracy: 0.6816\n",
      "Epoch 9/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5298 - accuracy: 0.7262 - val_loss: 0.6166 - val_accuracy: 0.6670\n",
      "Epoch 10/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.5243 - accuracy: 0.7339 - val_loss: 0.6217 - val_accuracy: 0.6627\n",
      "Epoch 11/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.5197 - accuracy: 0.7385 - val_loss: 0.6270 - val_accuracy: 0.6597\n",
      "Epoch 12/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.5162 - accuracy: 0.7405 - val_loss: 0.6331 - val_accuracy: 0.6493\n",
      "Epoch 13/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.5130 - accuracy: 0.7441 - val_loss: 0.6383 - val_accuracy: 0.6467\n",
      "Epoch 14/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5105 - accuracy: 0.7455 - val_loss: 0.6437 - val_accuracy: 0.6384\n",
      "Epoch 15/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5084 - accuracy: 0.7467 - val_loss: 0.6487 - val_accuracy: 0.6326\n",
      "Epoch 16/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5065 - accuracy: 0.7477 - val_loss: 0.6540 - val_accuracy: 0.6237\n",
      "Epoch 17/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.5049 - accuracy: 0.7481 - val_loss: 0.6580 - val_accuracy: 0.6207\n",
      "Epoch 18/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.5034 - accuracy: 0.7482 - val_loss: 0.6622 - val_accuracy: 0.6153\n",
      "Epoch 19/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.5020 - accuracy: 0.7499 - val_loss: 0.6650 - val_accuracy: 0.6164\n",
      "Epoch 20/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.5010 - accuracy: 0.7502 - val_loss: 0.6679 - val_accuracy: 0.6152\n",
      "Epoch 21/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.5000 - accuracy: 0.7509 - val_loss: 0.6709 - val_accuracy: 0.6095\n",
      "Epoch 22/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.4990 - accuracy: 0.7519 - val_loss: 0.6731 - val_accuracy: 0.6088\n",
      "Epoch 23/100\n",
      "387/387 [==============================] - 2s 6ms/step - loss: 0.4981 - accuracy: 0.7524 - val_loss: 0.6759 - val_accuracy: 0.6037\n",
      "Epoch 24/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4972 - accuracy: 0.7533 - val_loss: 0.6765 - val_accuracy: 0.6091\n",
      "Epoch 25/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4966 - accuracy: 0.7533 - val_loss: 0.6786 - val_accuracy: 0.6043\n",
      "Epoch 26/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4958 - accuracy: 0.7531 - val_loss: 0.6806 - val_accuracy: 0.5991\n",
      "Epoch 27/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4952 - accuracy: 0.7536 - val_loss: 0.6822 - val_accuracy: 0.5955\n",
      "Epoch 28/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4945 - accuracy: 0.7531 - val_loss: 0.6824 - val_accuracy: 0.5989\n",
      "Epoch 29/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4940 - accuracy: 0.7541 - val_loss: 0.6834 - val_accuracy: 0.5974\n",
      "Epoch 30/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4934 - accuracy: 0.7541 - val_loss: 0.6843 - val_accuracy: 0.5958\n",
      "Epoch 31/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4929 - accuracy: 0.7547 - val_loss: 0.6848 - val_accuracy: 0.5965\n",
      "Epoch 32/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4925 - accuracy: 0.7548 - val_loss: 0.6853 - val_accuracy: 0.5942\n",
      "Epoch 33/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4920 - accuracy: 0.7545 - val_loss: 0.6858 - val_accuracy: 0.5953\n",
      "Epoch 34/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4916 - accuracy: 0.7551 - val_loss: 0.6864 - val_accuracy: 0.5960\n",
      "Epoch 35/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4912 - accuracy: 0.7549 - val_loss: 0.6864 - val_accuracy: 0.5978\n",
      "Epoch 36/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4909 - accuracy: 0.7554 - val_loss: 0.6873 - val_accuracy: 0.5919\n",
      "Epoch 37/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4905 - accuracy: 0.7553 - val_loss: 0.6871 - val_accuracy: 0.5953\n",
      "Epoch 38/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4903 - accuracy: 0.7560 - val_loss: 0.6872 - val_accuracy: 0.5942\n",
      "Epoch 39/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4900 - accuracy: 0.7555 - val_loss: 0.6872 - val_accuracy: 0.5948\n",
      "Epoch 40/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4897 - accuracy: 0.7563 - val_loss: 0.6875 - val_accuracy: 0.5940\n",
      "Epoch 41/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4893 - accuracy: 0.7562 - val_loss: 0.6882 - val_accuracy: 0.5964\n",
      "Epoch 42/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4891 - accuracy: 0.7565 - val_loss: 0.6883 - val_accuracy: 0.5952\n",
      "Epoch 43/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4889 - accuracy: 0.7564 - val_loss: 0.6884 - val_accuracy: 0.5917\n",
      "Epoch 44/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4887 - accuracy: 0.7568 - val_loss: 0.6886 - val_accuracy: 0.5917\n",
      "Epoch 45/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4885 - accuracy: 0.7567 - val_loss: 0.6887 - val_accuracy: 0.5901\n",
      "Epoch 46/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4882 - accuracy: 0.7572 - val_loss: 0.6886 - val_accuracy: 0.5953\n",
      "Epoch 47/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4881 - accuracy: 0.7568 - val_loss: 0.6890 - val_accuracy: 0.5910\n",
      "Epoch 48/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4878 - accuracy: 0.7575 - val_loss: 0.6890 - val_accuracy: 0.5906\n",
      "Epoch 49/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4877 - accuracy: 0.7575 - val_loss: 0.6887 - val_accuracy: 0.5908\n",
      "Epoch 50/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4875 - accuracy: 0.7575 - val_loss: 0.6892 - val_accuracy: 0.5862\n",
      "Epoch 51/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4873 - accuracy: 0.7577 - val_loss: 0.6895 - val_accuracy: 0.5873\n",
      "Epoch 52/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4872 - accuracy: 0.7574 - val_loss: 0.6893 - val_accuracy: 0.5868\n",
      "Epoch 53/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4871 - accuracy: 0.7576 - val_loss: 0.6893 - val_accuracy: 0.5878\n",
      "Epoch 54/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4869 - accuracy: 0.7576 - val_loss: 0.6889 - val_accuracy: 0.5888\n",
      "Epoch 55/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4868 - accuracy: 0.7577 - val_loss: 0.6889 - val_accuracy: 0.5896\n",
      "Epoch 56/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4866 - accuracy: 0.7581 - val_loss: 0.6886 - val_accuracy: 0.5910\n",
      "Epoch 57/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4865 - accuracy: 0.7577 - val_loss: 0.6889 - val_accuracy: 0.5862\n",
      "Epoch 58/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4864 - accuracy: 0.7578 - val_loss: 0.6888 - val_accuracy: 0.5892\n",
      "Epoch 59/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4863 - accuracy: 0.7580 - val_loss: 0.6892 - val_accuracy: 0.5869\n",
      "Epoch 60/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4862 - accuracy: 0.7581 - val_loss: 0.6895 - val_accuracy: 0.5857\n",
      "Epoch 61/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4861 - accuracy: 0.7578 - val_loss: 0.6893 - val_accuracy: 0.5885\n",
      "Epoch 62/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4860 - accuracy: 0.7578 - val_loss: 0.6893 - val_accuracy: 0.5870\n",
      "Epoch 63/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4859 - accuracy: 0.7582 - val_loss: 0.6895 - val_accuracy: 0.5841\n",
      "Epoch 64/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4858 - accuracy: 0.7578 - val_loss: 0.6892 - val_accuracy: 0.5858\n",
      "Epoch 65/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4857 - accuracy: 0.7572 - val_loss: 0.6892 - val_accuracy: 0.5838\n",
      "Epoch 66/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4856 - accuracy: 0.7572 - val_loss: 0.6894 - val_accuracy: 0.5836\n",
      "Epoch 67/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4855 - accuracy: 0.7570 - val_loss: 0.6897 - val_accuracy: 0.5861\n",
      "Epoch 68/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4854 - accuracy: 0.7578 - val_loss: 0.6896 - val_accuracy: 0.5859\n",
      "Epoch 69/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4853 - accuracy: 0.7577 - val_loss: 0.6893 - val_accuracy: 0.5848\n",
      "Epoch 70/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4853 - accuracy: 0.7569 - val_loss: 0.6888 - val_accuracy: 0.5881\n",
      "Epoch 71/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4852 - accuracy: 0.7573 - val_loss: 0.6892 - val_accuracy: 0.5847\n",
      "Epoch 72/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4852 - accuracy: 0.7561 - val_loss: 0.6894 - val_accuracy: 0.5836\n",
      "Epoch 73/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4851 - accuracy: 0.7570 - val_loss: 0.6895 - val_accuracy: 0.5851\n",
      "Epoch 74/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4851 - accuracy: 0.7570 - val_loss: 0.6897 - val_accuracy: 0.5846\n",
      "Epoch 75/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4850 - accuracy: 0.7574 - val_loss: 0.6901 - val_accuracy: 0.5816\n",
      "Epoch 76/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4849 - accuracy: 0.7572 - val_loss: 0.6901 - val_accuracy: 0.5824\n",
      "Epoch 77/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4849 - accuracy: 0.7570 - val_loss: 0.6901 - val_accuracy: 0.5852\n",
      "Epoch 78/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4848 - accuracy: 0.7570 - val_loss: 0.6902 - val_accuracy: 0.5832\n",
      "Epoch 79/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4848 - accuracy: 0.7568 - val_loss: 0.6901 - val_accuracy: 0.5863\n",
      "Epoch 80/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4847 - accuracy: 0.7572 - val_loss: 0.6903 - val_accuracy: 0.5822\n",
      "Epoch 81/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4846 - accuracy: 0.7568 - val_loss: 0.6902 - val_accuracy: 0.5858\n",
      "Epoch 82/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4846 - accuracy: 0.7569 - val_loss: 0.6904 - val_accuracy: 0.5846\n",
      "Epoch 83/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4846 - accuracy: 0.7571 - val_loss: 0.6903 - val_accuracy: 0.5843\n",
      "Epoch 84/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4845 - accuracy: 0.7572 - val_loss: 0.6904 - val_accuracy: 0.5850\n",
      "Epoch 85/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4845 - accuracy: 0.7569 - val_loss: 0.6902 - val_accuracy: 0.5846\n",
      "Epoch 86/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4845 - accuracy: 0.7569 - val_loss: 0.6900 - val_accuracy: 0.5838\n",
      "Epoch 87/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4844 - accuracy: 0.7569 - val_loss: 0.6899 - val_accuracy: 0.5845\n",
      "Epoch 88/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4844 - accuracy: 0.7569 - val_loss: 0.6902 - val_accuracy: 0.5850\n",
      "Epoch 89/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4843 - accuracy: 0.7568 - val_loss: 0.6901 - val_accuracy: 0.5831\n",
      "Epoch 90/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4843 - accuracy: 0.7567 - val_loss: 0.6904 - val_accuracy: 0.5839\n",
      "Epoch 91/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4843 - accuracy: 0.7566 - val_loss: 0.6905 - val_accuracy: 0.5824\n",
      "Epoch 92/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4843 - accuracy: 0.7568 - val_loss: 0.6906 - val_accuracy: 0.5832\n",
      "Epoch 93/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4842 - accuracy: 0.7569 - val_loss: 0.6902 - val_accuracy: 0.5828\n",
      "Epoch 94/100\n",
      "387/387 [==============================] - 2s 5ms/step - loss: 0.4842 - accuracy: 0.7572 - val_loss: 0.6905 - val_accuracy: 0.5822\n",
      "Epoch 95/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4841 - accuracy: 0.7570 - val_loss: 0.6906 - val_accuracy: 0.5823\n",
      "Epoch 96/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4841 - accuracy: 0.7567 - val_loss: 0.6904 - val_accuracy: 0.5831\n",
      "Epoch 97/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4840 - accuracy: 0.7568 - val_loss: 0.6906 - val_accuracy: 0.5838\n",
      "Epoch 98/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4840 - accuracy: 0.7568 - val_loss: 0.6904 - val_accuracy: 0.5835\n",
      "Epoch 99/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4840 - accuracy: 0.7571 - val_loss: 0.6902 - val_accuracy: 0.5838\n",
      "Epoch 100/100\n",
      "387/387 [==============================] - 2s 4ms/step - loss: 0.4840 - accuracy: 0.7573 - val_loss: 0.6901 - val_accuracy: 0.5823\n",
      "Epoch 1/100\n",
      "387/387 [==============================] - 6s 13ms/step - loss: 0.6092 - accuracy: 0.7035 - val_loss: 0.6031 - val_accuracy: 0.7018\n",
      "Epoch 2/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.5757 - accuracy: 0.7037 - val_loss: 0.6014 - val_accuracy: 0.7014\n",
      "Epoch 3/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.5531 - accuracy: 0.7131 - val_loss: 0.6069 - val_accuracy: 0.6874\n",
      "Epoch 4/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.5362 - accuracy: 0.7210 - val_loss: 0.6171 - val_accuracy: 0.6719\n",
      "Epoch 5/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.5243 - accuracy: 0.7338 - val_loss: 0.6298 - val_accuracy: 0.6548\n",
      "Epoch 6/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.5159 - accuracy: 0.7407 - val_loss: 0.6425 - val_accuracy: 0.6383\n",
      "Epoch 7/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.5100 - accuracy: 0.7464 - val_loss: 0.6549 - val_accuracy: 0.6254\n",
      "Epoch 8/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.5054 - accuracy: 0.7481 - val_loss: 0.6620 - val_accuracy: 0.6218\n",
      "Epoch 9/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.5012 - accuracy: 0.7499 - val_loss: 0.6685 - val_accuracy: 0.6154\n",
      "Epoch 10/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4975 - accuracy: 0.7515 - val_loss: 0.6754 - val_accuracy: 0.5998\n",
      "Epoch 11/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4943 - accuracy: 0.7540 - val_loss: 0.6782 - val_accuracy: 0.6035\n",
      "Epoch 12/100\n",
      "387/387 [==============================] - 4s 12ms/step - loss: 0.4908 - accuracy: 0.7558 - val_loss: 0.6828 - val_accuracy: 0.6004\n",
      "Epoch 13/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4875 - accuracy: 0.7574 - val_loss: 0.6853 - val_accuracy: 0.5979\n",
      "Epoch 14/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4842 - accuracy: 0.7592 - val_loss: 0.6910 - val_accuracy: 0.5860\n",
      "Epoch 15/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4806 - accuracy: 0.7630 - val_loss: 0.6883 - val_accuracy: 0.5976\n",
      "Epoch 16/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4771 - accuracy: 0.7650 - val_loss: 0.6950 - val_accuracy: 0.5860\n",
      "Epoch 17/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4734 - accuracy: 0.7674 - val_loss: 0.6934 - val_accuracy: 0.5922\n",
      "Epoch 18/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4698 - accuracy: 0.7705 - val_loss: 0.6962 - val_accuracy: 0.5965\n",
      "Epoch 19/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4661 - accuracy: 0.7744 - val_loss: 0.6974 - val_accuracy: 0.5986\n",
      "Epoch 20/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4625 - accuracy: 0.7767 - val_loss: 0.6996 - val_accuracy: 0.5979\n",
      "Epoch 21/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4589 - accuracy: 0.7792 - val_loss: 0.7019 - val_accuracy: 0.5972\n",
      "Epoch 22/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4554 - accuracy: 0.7829 - val_loss: 0.7074 - val_accuracy: 0.5929\n",
      "Epoch 23/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4519 - accuracy: 0.7852 - val_loss: 0.7109 - val_accuracy: 0.5971\n",
      "Epoch 24/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.4487 - accuracy: 0.7876 - val_loss: 0.7127 - val_accuracy: 0.5990\n",
      "Epoch 25/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4457 - accuracy: 0.7952 - val_loss: 0.7154 - val_accuracy: 0.6095\n",
      "Epoch 26/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4426 - accuracy: 0.8000 - val_loss: 0.7202 - val_accuracy: 0.6089\n",
      "Epoch 27/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4399 - accuracy: 0.8022 - val_loss: 0.7188 - val_accuracy: 0.6140\n",
      "Epoch 28/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4371 - accuracy: 0.8031 - val_loss: 0.7219 - val_accuracy: 0.6113\n",
      "Epoch 29/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4346 - accuracy: 0.8047 - val_loss: 0.7235 - val_accuracy: 0.6170\n",
      "Epoch 30/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4322 - accuracy: 0.8063 - val_loss: 0.7307 - val_accuracy: 0.6145\n",
      "Epoch 31/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4299 - accuracy: 0.8076 - val_loss: 0.7338 - val_accuracy: 0.6149\n",
      "Epoch 32/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4277 - accuracy: 0.8090 - val_loss: 0.7402 - val_accuracy: 0.6134\n",
      "Epoch 33/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.4257 - accuracy: 0.8100 - val_loss: 0.7403 - val_accuracy: 0.6141\n",
      "Epoch 34/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.4239 - accuracy: 0.8115 - val_loss: 0.7467 - val_accuracy: 0.6121\n",
      "Epoch 35/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4219 - accuracy: 0.8121 - val_loss: 0.7503 - val_accuracy: 0.6132\n",
      "Epoch 36/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4202 - accuracy: 0.8136 - val_loss: 0.7537 - val_accuracy: 0.6123\n",
      "Epoch 37/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4184 - accuracy: 0.8152 - val_loss: 0.7587 - val_accuracy: 0.6142\n",
      "Epoch 38/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4169 - accuracy: 0.8164 - val_loss: 0.7608 - val_accuracy: 0.6137\n",
      "Epoch 39/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4153 - accuracy: 0.8168 - val_loss: 0.7624 - val_accuracy: 0.6164\n",
      "Epoch 40/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4140 - accuracy: 0.8177 - val_loss: 0.7681 - val_accuracy: 0.6133\n",
      "Epoch 41/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4125 - accuracy: 0.8191 - val_loss: 0.7709 - val_accuracy: 0.6157\n",
      "Epoch 42/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4112 - accuracy: 0.8201 - val_loss: 0.7751 - val_accuracy: 0.6139\n",
      "Epoch 43/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.4100 - accuracy: 0.8201 - val_loss: 0.7775 - val_accuracy: 0.6141\n",
      "Epoch 44/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4088 - accuracy: 0.8209 - val_loss: 0.7829 - val_accuracy: 0.6110\n",
      "Epoch 45/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4076 - accuracy: 0.8215 - val_loss: 0.7812 - val_accuracy: 0.6150\n",
      "Epoch 46/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4065 - accuracy: 0.8220 - val_loss: 0.7928 - val_accuracy: 0.6086\n",
      "Epoch 47/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.4052 - accuracy: 0.8227 - val_loss: 0.7881 - val_accuracy: 0.6126\n",
      "Epoch 48/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.4042 - accuracy: 0.8227 - val_loss: 0.7904 - val_accuracy: 0.6127\n",
      "Epoch 49/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4031 - accuracy: 0.8239 - val_loss: 0.7952 - val_accuracy: 0.6136\n",
      "Epoch 50/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4022 - accuracy: 0.8245 - val_loss: 0.8005 - val_accuracy: 0.6116\n",
      "Epoch 51/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.4012 - accuracy: 0.8248 - val_loss: 0.8029 - val_accuracy: 0.6109\n",
      "Epoch 52/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.4003 - accuracy: 0.8255 - val_loss: 0.8011 - val_accuracy: 0.6119\n",
      "Epoch 53/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.3994 - accuracy: 0.8261 - val_loss: 0.8068 - val_accuracy: 0.6104\n",
      "Epoch 54/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.3985 - accuracy: 0.8270 - val_loss: 0.8076 - val_accuracy: 0.6122\n",
      "Epoch 55/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3977 - accuracy: 0.8274 - val_loss: 0.8088 - val_accuracy: 0.6112\n",
      "Epoch 56/100\n",
      "387/387 [==============================] - 6s 17ms/step - loss: 0.3968 - accuracy: 0.8283 - val_loss: 0.8111 - val_accuracy: 0.6124\n",
      "Epoch 57/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3961 - accuracy: 0.8285 - val_loss: 0.8165 - val_accuracy: 0.6111\n",
      "Epoch 58/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.3953 - accuracy: 0.8290 - val_loss: 0.8214 - val_accuracy: 0.6116\n",
      "Epoch 59/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.3948 - accuracy: 0.8295 - val_loss: 0.8230 - val_accuracy: 0.6107\n",
      "Epoch 60/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3941 - accuracy: 0.8293 - val_loss: 0.8212 - val_accuracy: 0.6116\n",
      "Epoch 61/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.3934 - accuracy: 0.8302 - val_loss: 0.8315 - val_accuracy: 0.6088\n",
      "Epoch 62/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3928 - accuracy: 0.8296 - val_loss: 0.8287 - val_accuracy: 0.6104\n",
      "Epoch 63/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.3922 - accuracy: 0.8305 - val_loss: 0.8318 - val_accuracy: 0.6106\n",
      "Epoch 64/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3916 - accuracy: 0.8308 - val_loss: 0.8310 - val_accuracy: 0.6129\n",
      "Epoch 65/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3909 - accuracy: 0.8319 - val_loss: 0.8261 - val_accuracy: 0.6161\n",
      "Epoch 66/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3905 - accuracy: 0.8319 - val_loss: 0.8329 - val_accuracy: 0.6129\n",
      "Epoch 67/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.3898 - accuracy: 0.8324 - val_loss: 0.8391 - val_accuracy: 0.6121\n",
      "Epoch 68/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3893 - accuracy: 0.8330 - val_loss: 0.8423 - val_accuracy: 0.6122\n",
      "Epoch 69/100\n",
      "387/387 [==============================] - 4s 12ms/step - loss: 0.3888 - accuracy: 0.8329 - val_loss: 0.8424 - val_accuracy: 0.6109\n",
      "Epoch 70/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.3883 - accuracy: 0.8336 - val_loss: 0.8406 - val_accuracy: 0.6143\n",
      "Epoch 71/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.3878 - accuracy: 0.8341 - val_loss: 0.8408 - val_accuracy: 0.6133\n",
      "Epoch 72/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.3873 - accuracy: 0.8347 - val_loss: 0.8477 - val_accuracy: 0.6113\n",
      "Epoch 73/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.3868 - accuracy: 0.8348 - val_loss: 0.8493 - val_accuracy: 0.6108\n",
      "Epoch 74/100\n",
      "387/387 [==============================] - 6s 14ms/step - loss: 0.3862 - accuracy: 0.8348 - val_loss: 0.8514 - val_accuracy: 0.6128\n",
      "Epoch 75/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3858 - accuracy: 0.8354 - val_loss: 0.8473 - val_accuracy: 0.6131\n",
      "Epoch 76/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3854 - accuracy: 0.8356 - val_loss: 0.8464 - val_accuracy: 0.6142\n",
      "Epoch 77/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3849 - accuracy: 0.8358 - val_loss: 0.8517 - val_accuracy: 0.6120\n",
      "Epoch 78/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3844 - accuracy: 0.8363 - val_loss: 0.8565 - val_accuracy: 0.6120\n",
      "Epoch 79/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3840 - accuracy: 0.8366 - val_loss: 0.8552 - val_accuracy: 0.6123\n",
      "Epoch 80/100\n",
      "387/387 [==============================] - 4s 10ms/step - loss: 0.3836 - accuracy: 0.8372 - val_loss: 0.8594 - val_accuracy: 0.6125\n",
      "Epoch 81/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3831 - accuracy: 0.8374 - val_loss: 0.8635 - val_accuracy: 0.6121\n",
      "Epoch 82/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3828 - accuracy: 0.8375 - val_loss: 0.8590 - val_accuracy: 0.6143\n",
      "Epoch 83/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3824 - accuracy: 0.8381 - val_loss: 0.8633 - val_accuracy: 0.6128\n",
      "Epoch 84/100\n",
      "387/387 [==============================] - 6s 14ms/step - loss: 0.3820 - accuracy: 0.8382 - val_loss: 0.8554 - val_accuracy: 0.6167\n",
      "Epoch 85/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.3816 - accuracy: 0.8388 - val_loss: 0.8595 - val_accuracy: 0.6150\n",
      "Epoch 86/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3812 - accuracy: 0.8388 - val_loss: 0.8610 - val_accuracy: 0.6143\n",
      "Epoch 87/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3808 - accuracy: 0.8389 - val_loss: 0.8640 - val_accuracy: 0.6138\n",
      "Epoch 88/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3805 - accuracy: 0.8394 - val_loss: 0.8595 - val_accuracy: 0.6156\n",
      "Epoch 89/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3800 - accuracy: 0.8398 - val_loss: 0.8634 - val_accuracy: 0.6150\n",
      "Epoch 90/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.3797 - accuracy: 0.8395 - val_loss: 0.8657 - val_accuracy: 0.6147\n",
      "Epoch 91/100\n",
      "387/387 [==============================] - 8s 21ms/step - loss: 0.3793 - accuracy: 0.8398 - val_loss: 0.8690 - val_accuracy: 0.6123\n",
      "Epoch 92/100\n",
      "387/387 [==============================] - 10s 25ms/step - loss: 0.3790 - accuracy: 0.8404 - val_loss: 0.8648 - val_accuracy: 0.6153\n",
      "Epoch 93/100\n",
      "387/387 [==============================] - 8s 21ms/step - loss: 0.3786 - accuracy: 0.8406 - val_loss: 0.8722 - val_accuracy: 0.6135\n",
      "Epoch 94/100\n",
      "387/387 [==============================] - 7s 18ms/step - loss: 0.3783 - accuracy: 0.8405 - val_loss: 0.8696 - val_accuracy: 0.6139\n",
      "Epoch 95/100\n",
      "387/387 [==============================] - 7s 17ms/step - loss: 0.3779 - accuracy: 0.8412 - val_loss: 0.8723 - val_accuracy: 0.6139\n",
      "Epoch 96/100\n",
      "387/387 [==============================] - 7s 18ms/step - loss: 0.3777 - accuracy: 0.8408 - val_loss: 0.8756 - val_accuracy: 0.6126\n",
      "Epoch 97/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.3774 - accuracy: 0.8412 - val_loss: 0.8739 - val_accuracy: 0.6135\n",
      "Epoch 98/100\n",
      "387/387 [==============================] - 6s 17ms/step - loss: 0.3770 - accuracy: 0.8416 - val_loss: 0.8769 - val_accuracy: 0.6145\n",
      "Epoch 99/100\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.3768 - accuracy: 0.8413 - val_loss: 0.8726 - val_accuracy: 0.6153\n",
      "Epoch 100/100\n",
      "387/387 [==============================] - 7s 18ms/step - loss: 0.3765 - accuracy: 0.8421 - val_loss: 0.8740 - val_accuracy: 0.6157\n",
      "Epoch 1/100\n",
      "387/387 [==============================] - 9s 21ms/step - loss: 0.6195 - accuracy: 0.7031 - val_loss: 0.6057 - val_accuracy: 0.7018\n",
      "Epoch 2/100\n",
      "387/387 [==============================] - 7s 17ms/step - loss: 0.5887 - accuracy: 0.7037 - val_loss: 0.6017 - val_accuracy: 0.7018\n",
      "Epoch 3/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.5672 - accuracy: 0.7058 - val_loss: 0.6024 - val_accuracy: 0.7011\n",
      "Epoch 4/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.5457 - accuracy: 0.7200 - val_loss: 0.6131 - val_accuracy: 0.6754\n",
      "Epoch 5/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.5292 - accuracy: 0.7283 - val_loss: 0.6261 - val_accuracy: 0.6617\n",
      "Epoch 6/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.5179 - accuracy: 0.7393 - val_loss: 0.6402 - val_accuracy: 0.6527\n",
      "Epoch 7/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.5096 - accuracy: 0.7453 - val_loss: 0.6549 - val_accuracy: 0.6389\n",
      "Epoch 8/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.5030 - accuracy: 0.7498 - val_loss: 0.6663 - val_accuracy: 0.6309\n",
      "Epoch 9/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.4973 - accuracy: 0.7534 - val_loss: 0.6764 - val_accuracy: 0.6243\n",
      "Epoch 10/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.4918 - accuracy: 0.7572 - val_loss: 0.6843 - val_accuracy: 0.6218\n",
      "Epoch 11/100\n",
      "387/387 [==============================] - 6s 14ms/step - loss: 0.4870 - accuracy: 0.7615 - val_loss: 0.6863 - val_accuracy: 0.6257\n",
      "Epoch 12/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.4824 - accuracy: 0.7646 - val_loss: 0.6892 - val_accuracy: 0.6256\n",
      "Epoch 13/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.4778 - accuracy: 0.7683 - val_loss: 0.6933 - val_accuracy: 0.6245\n",
      "Epoch 14/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.4734 - accuracy: 0.7708 - val_loss: 0.7014 - val_accuracy: 0.6201\n",
      "Epoch 15/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.4692 - accuracy: 0.7745 - val_loss: 0.7022 - val_accuracy: 0.6218\n",
      "Epoch 16/100\n",
      "387/387 [==============================] - 7s 18ms/step - loss: 0.4651 - accuracy: 0.7777 - val_loss: 0.7053 - val_accuracy: 0.6208\n",
      "Epoch 17/100\n",
      "387/387 [==============================] - 8s 20ms/step - loss: 0.4610 - accuracy: 0.7811 - val_loss: 0.7054 - val_accuracy: 0.6240\n",
      "Epoch 18/100\n",
      "387/387 [==============================] - 7s 17ms/step - loss: 0.4573 - accuracy: 0.7837 - val_loss: 0.7117 - val_accuracy: 0.6197\n",
      "Epoch 19/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.4537 - accuracy: 0.7862 - val_loss: 0.7185 - val_accuracy: 0.6143\n",
      "Epoch 20/100\n",
      "387/387 [==============================] - 7s 17ms/step - loss: 0.4500 - accuracy: 0.7886 - val_loss: 0.7158 - val_accuracy: 0.6202\n",
      "Epoch 21/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.4466 - accuracy: 0.7907 - val_loss: 0.7291 - val_accuracy: 0.6099\n",
      "Epoch 22/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.4433 - accuracy: 0.7925 - val_loss: 0.7278 - val_accuracy: 0.6119\n",
      "Epoch 23/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.4400 - accuracy: 0.7949 - val_loss: 0.7284 - val_accuracy: 0.6163\n",
      "Epoch 24/100\n",
      "387/387 [==============================] - 7s 18ms/step - loss: 0.4370 - accuracy: 0.7963 - val_loss: 0.7306 - val_accuracy: 0.6176\n",
      "Epoch 25/100\n",
      "387/387 [==============================] - 6s 14ms/step - loss: 0.4341 - accuracy: 0.7992 - val_loss: 0.7354 - val_accuracy: 0.6144\n",
      "Epoch 26/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4311 - accuracy: 0.8005 - val_loss: 0.7370 - val_accuracy: 0.6174\n",
      "Epoch 27/100\n",
      "387/387 [==============================] - 6s 16ms/step - loss: 0.4284 - accuracy: 0.8020 - val_loss: 0.7412 - val_accuracy: 0.6173\n",
      "Epoch 28/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4258 - accuracy: 0.8031 - val_loss: 0.7469 - val_accuracy: 0.6141\n",
      "Epoch 29/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.4233 - accuracy: 0.8050 - val_loss: 0.7558 - val_accuracy: 0.6118\n",
      "Epoch 30/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.4209 - accuracy: 0.8066 - val_loss: 0.7519 - val_accuracy: 0.6190\n",
      "Epoch 31/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.4187 - accuracy: 0.8081 - val_loss: 0.7649 - val_accuracy: 0.6124\n",
      "Epoch 32/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.4164 - accuracy: 0.8092 - val_loss: 0.7630 - val_accuracy: 0.6164\n",
      "Epoch 33/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4143 - accuracy: 0.8107 - val_loss: 0.7716 - val_accuracy: 0.6143\n",
      "Epoch 34/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4122 - accuracy: 0.8120 - val_loss: 0.7753 - val_accuracy: 0.6149\n",
      "Epoch 35/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4104 - accuracy: 0.8124 - val_loss: 0.7724 - val_accuracy: 0.6180\n",
      "Epoch 36/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4084 - accuracy: 0.8139 - val_loss: 0.7842 - val_accuracy: 0.6115\n",
      "Epoch 37/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4067 - accuracy: 0.8160 - val_loss: 0.7807 - val_accuracy: 0.6166\n",
      "Epoch 38/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.4049 - accuracy: 0.8164 - val_loss: 0.7944 - val_accuracy: 0.6131\n",
      "Epoch 39/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.4030 - accuracy: 0.8180 - val_loss: 0.7966 - val_accuracy: 0.6129\n",
      "Epoch 40/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.4013 - accuracy: 0.8190 - val_loss: 0.7966 - val_accuracy: 0.6162\n",
      "Epoch 41/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.3998 - accuracy: 0.8209 - val_loss: 0.8090 - val_accuracy: 0.6113\n",
      "Epoch 42/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3982 - accuracy: 0.8204 - val_loss: 0.8109 - val_accuracy: 0.6148\n",
      "Epoch 43/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3969 - accuracy: 0.8215 - val_loss: 0.8100 - val_accuracy: 0.6165\n",
      "Epoch 44/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3954 - accuracy: 0.8228 - val_loss: 0.8123 - val_accuracy: 0.6158\n",
      "Epoch 45/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3938 - accuracy: 0.8233 - val_loss: 0.8211 - val_accuracy: 0.6110\n",
      "Epoch 46/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3924 - accuracy: 0.8238 - val_loss: 0.8255 - val_accuracy: 0.6134\n",
      "Epoch 47/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3912 - accuracy: 0.8252 - val_loss: 0.8270 - val_accuracy: 0.6118\n",
      "Epoch 48/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3898 - accuracy: 0.8261 - val_loss: 0.8262 - val_accuracy: 0.6194\n",
      "Epoch 49/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3886 - accuracy: 0.8270 - val_loss: 0.8365 - val_accuracy: 0.6139\n",
      "Epoch 50/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3873 - accuracy: 0.8277 - val_loss: 0.8450 - val_accuracy: 0.6103\n",
      "Epoch 51/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3860 - accuracy: 0.8288 - val_loss: 0.8412 - val_accuracy: 0.6160\n",
      "Epoch 52/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3848 - accuracy: 0.8294 - val_loss: 0.8499 - val_accuracy: 0.6113\n",
      "Epoch 53/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3837 - accuracy: 0.8301 - val_loss: 0.8511 - val_accuracy: 0.6134\n",
      "Epoch 54/100\n",
      "387/387 [==============================] - 6s 15ms/step - loss: 0.3825 - accuracy: 0.8309 - val_loss: 0.8538 - val_accuracy: 0.6127\n",
      "Epoch 55/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3814 - accuracy: 0.8315 - val_loss: 0.8640 - val_accuracy: 0.6084\n",
      "Epoch 56/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3803 - accuracy: 0.8328 - val_loss: 0.8633 - val_accuracy: 0.6098\n",
      "Epoch 57/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3792 - accuracy: 0.8333 - val_loss: 0.8717 - val_accuracy: 0.6078\n",
      "Epoch 58/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3783 - accuracy: 0.8334 - val_loss: 0.8749 - val_accuracy: 0.6094\n",
      "Epoch 59/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3772 - accuracy: 0.8341 - val_loss: 0.8672 - val_accuracy: 0.6118\n",
      "Epoch 60/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3762 - accuracy: 0.8358 - val_loss: 0.8734 - val_accuracy: 0.6093\n",
      "Epoch 61/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3753 - accuracy: 0.8356 - val_loss: 0.8829 - val_accuracy: 0.6095\n",
      "Epoch 62/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3744 - accuracy: 0.8367 - val_loss: 0.8910 - val_accuracy: 0.6076\n",
      "Epoch 63/100\n",
      "387/387 [==============================] - 4s 12ms/step - loss: 0.3735 - accuracy: 0.8377 - val_loss: 0.8901 - val_accuracy: 0.6106\n",
      "Epoch 64/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3727 - accuracy: 0.8374 - val_loss: 0.8845 - val_accuracy: 0.6114\n",
      "Epoch 65/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3719 - accuracy: 0.8381 - val_loss: 0.9002 - val_accuracy: 0.6062\n",
      "Epoch 66/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3709 - accuracy: 0.8385 - val_loss: 0.8908 - val_accuracy: 0.6125\n",
      "Epoch 67/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3702 - accuracy: 0.8391 - val_loss: 0.9082 - val_accuracy: 0.6055\n",
      "Epoch 68/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3695 - accuracy: 0.8394 - val_loss: 0.9105 - val_accuracy: 0.6064\n",
      "Epoch 69/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3687 - accuracy: 0.8395 - val_loss: 0.8973 - val_accuracy: 0.6121\n",
      "Epoch 70/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3678 - accuracy: 0.8400 - val_loss: 0.9158 - val_accuracy: 0.6078\n",
      "Epoch 71/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3671 - accuracy: 0.8409 - val_loss: 0.9140 - val_accuracy: 0.6096\n",
      "Epoch 72/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3664 - accuracy: 0.8411 - val_loss: 0.9138 - val_accuracy: 0.6101\n",
      "Epoch 73/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3657 - accuracy: 0.8419 - val_loss: 0.9214 - val_accuracy: 0.6078\n",
      "Epoch 74/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3650 - accuracy: 0.8421 - val_loss: 0.9199 - val_accuracy: 0.6099\n",
      "Epoch 75/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3642 - accuracy: 0.8426 - val_loss: 0.9235 - val_accuracy: 0.6093\n",
      "Epoch 76/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3635 - accuracy: 0.8428 - val_loss: 0.9228 - val_accuracy: 0.6111\n",
      "Epoch 77/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3627 - accuracy: 0.8436 - val_loss: 0.9251 - val_accuracy: 0.6109\n",
      "Epoch 78/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3622 - accuracy: 0.8434 - val_loss: 0.9340 - val_accuracy: 0.6089\n",
      "Epoch 79/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3614 - accuracy: 0.8445 - val_loss: 0.9433 - val_accuracy: 0.6071\n",
      "Epoch 80/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3606 - accuracy: 0.8448 - val_loss: 0.9380 - val_accuracy: 0.6091\n",
      "Epoch 81/100\n",
      "387/387 [==============================] - 5s 14ms/step - loss: 0.3601 - accuracy: 0.8451 - val_loss: 0.9411 - val_accuracy: 0.6097\n",
      "Epoch 82/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3595 - accuracy: 0.8458 - val_loss: 0.9406 - val_accuracy: 0.6100\n",
      "Epoch 83/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3587 - accuracy: 0.8455 - val_loss: 0.9396 - val_accuracy: 0.6092\n",
      "Epoch 84/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3577 - accuracy: 0.8463 - val_loss: 0.9413 - val_accuracy: 0.6107\n",
      "Epoch 85/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3563 - accuracy: 0.8467 - val_loss: 0.9388 - val_accuracy: 0.6081\n",
      "Epoch 86/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3550 - accuracy: 0.8469 - val_loss: 0.9422 - val_accuracy: 0.6104\n",
      "Epoch 87/100\n",
      "387/387 [==============================] - 4s 12ms/step - loss: 0.3533 - accuracy: 0.8488 - val_loss: 0.9539 - val_accuracy: 0.6031\n",
      "Epoch 88/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3517 - accuracy: 0.8530 - val_loss: 0.9505 - val_accuracy: 0.6172\n",
      "Epoch 89/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3499 - accuracy: 0.8582 - val_loss: 0.9419 - val_accuracy: 0.6172\n",
      "Epoch 90/100\n",
      "387/387 [==============================] - 4s 12ms/step - loss: 0.3480 - accuracy: 0.8598 - val_loss: 0.9467 - val_accuracy: 0.6129\n",
      "Epoch 91/100\n",
      "387/387 [==============================] - 4s 11ms/step - loss: 0.3459 - accuracy: 0.8613 - val_loss: 0.9445 - val_accuracy: 0.6202\n",
      "Epoch 92/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3441 - accuracy: 0.8619 - val_loss: 0.9490 - val_accuracy: 0.6120\n",
      "Epoch 93/100\n",
      "387/387 [==============================] - 4s 12ms/step - loss: 0.3418 - accuracy: 0.8637 - val_loss: 0.9515 - val_accuracy: 0.6140\n",
      "Epoch 94/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3397 - accuracy: 0.8642 - val_loss: 0.9651 - val_accuracy: 0.6089\n",
      "Epoch 95/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3375 - accuracy: 0.8656 - val_loss: 0.9507 - val_accuracy: 0.6200\n",
      "Epoch 96/100\n",
      "387/387 [==============================] - 5s 13ms/step - loss: 0.3356 - accuracy: 0.8666 - val_loss: 0.9719 - val_accuracy: 0.6053\n",
      "Epoch 97/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3334 - accuracy: 0.8674 - val_loss: 0.9733 - val_accuracy: 0.6080\n",
      "Epoch 98/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3312 - accuracy: 0.8686 - val_loss: 0.9646 - val_accuracy: 0.6108\n",
      "Epoch 99/100\n",
      "387/387 [==============================] - 5s 12ms/step - loss: 0.3293 - accuracy: 0.8692 - val_loss: 0.9783 - val_accuracy: 0.6045\n",
      "Epoch 100/100\n",
      "387/387 [==============================] - 7s 18ms/step - loss: 0.3272 - accuracy: 0.8707 - val_loss: 0.9626 - val_accuracy: 0.6156\n"
     ]
    }
   ],
   "source": [
    "#fitting each model with the training data\n",
    "history1HL = model1HL.fit(train_data,\n",
    "                    np.array(train_labels),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(devtest_data, np.array(devtest_labels)))\n",
    "history5HL = model5HL.fit(train_data,\n",
    "                    np.array(train_labels),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(devtest_data, np.array(devtest_labels)))\n",
    "                    \n",
    "history7HL = model7HL.fit(train_data,\n",
    "                    np.array(train_labels),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(devtest_data, np.array(devtest_labels)))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation accuracy with 1 hidden layer\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation accuracy with 5 hidden layers\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA3HUlEQVR4nO3deXRTZfrA8e9DWcsqBRFBKIwoglgoFZVNNhVXREVBdFicUVQEddxmEHEcmTPj+Bt3VNxQxKkrjOMoKoKDuxQBZVVABESQTVaBQp/fH+9Nm4akTdqkSZPnc05Okru+N03fJ+96RVUxxhhjAlWJdwKMMcYkJgsQxhhjgrIAYYwxJigLEMYYY4KyAGGMMSYoCxDGGGOCsgBhwiYi74jIsGhvG08iskZE+sXguCoix3qvnxCR8eFsW4bzDBWR98qaTmNKIjYOIrmJyG6/t+nAfuCQ9/4aVZ1W8alKHCKyBvidqs6K8nEVaKOqK6O1rYhkAt8D1VT1YFQSakwJqsY7ASa2VLWO73VJmaGIVLVMxyQK+z4mBqtiSlEi0ktE1ovI7SKyEXhORI4QkbdEZLOIbPdeN/fb50MR+Z33eriIfCwi93vbfi8iZ5dx21YiMldEdonILBF5TEReDJHucNL4FxH5xDveeyLSyG/9lSLyg4hsFZFxJXw+p4rIRhFJ81s2UES+9l53EZHPROQXEflJRB4VkeohjjVFRO71e3+rt88GERkZsO25IrJARHaKyDoRudtv9Vzv+RcR2S0ip/k+W7/9u4rIPBHZ4T13DfezifBzbigiz3nXsF1EZvitGyAiC71rWCUi/b3lxarzRORu399ZRDK9qrarRGQtMNtb/qr3d9jhfUfa++1fS0T+z/t77vC+Y7VE5L8ickPA9XwtIhcGu1YTmgWI1HYU0BBoCVyN+z48571vAfwKPFrC/qcAK4BGwH3AMyIiZdj2JeBLIAO4G7iyhHOGk8bLgRHAkUB14BYAEWkHPO4d/2jvfM0JQlU/B/YAfQKO+5L3+hBwk3c9pwF9getKSDdeGvp76TkDaAMEtn/sAX4LNADOBa71y9h6es8NVLWOqn4WcOyGwH+Bh71r+yfwXxHJCLiGwz6bIEr7nKfiqizbe8d6wEtDF+AF4FbvGnoCa0KcI5jTgROAs7z37+A+pyOBrwD/KtH7gc5AV9z3+DagAHgeuMK3kYhkAc2AtyNIhwFQVXukyAP3j9rPe90LOADULGH7jsB2v/cf4qqoAIYDK/3WpQMKHBXJtrjM5yCQ7rf+ReDFMK8pWBrv9Ht/HTDTe30XkOu3rrb3GfQLcex7gWe913VxmXfLENveCEz3e6/Asd7rKcC93utngb/5bXec/7ZBjvsg8ID3OtPbtqrf+uHAx97rK4EvA/b/DBhe2mcTyecMNMVlxEcE2e5JX3pL+v557+/2/Z39rq11CWlo4G1THxfAfgWygmxXA9iGa9cBF0gmxeJ/KtkfVoJIbZtVdZ/vjYiki8iTXpF9J65Ko4F/NUuAjb4XqrrXe1knwm2PBrb5LQNYFyrBYaZxo9/rvX5pOtr/2Kq6B9ga6ly40sJFIlIDuAj4SlV/8NJxnFftstFLx19xpYnSFEsD8EPA9Z0iInO8qp0dwKgwj+s79g8By37A/Xr2CfXZFFPK53wM7m+2PciuxwCrwkxvMIWfjYikicjfvGqqnRSVRBp5j5rBzqWq+4FXgCtEpAowBFfiMRGyAJHaAruw/QE4HjhFVetRVKURqtooGn4CGopIut+yY0rYvjxp/Mn/2N45M0JtrKpLcRns2RSvXgJXVbUc9yu1HvCnsqQBV4Ly9xLwJnCMqtYHnvA7bmldDjfgqoT8tQB+DCNdgUr6nNfh/mYNguy3DvhNiGPuwZUefY4Kso3/NV4ODMBVw9XHlTJ8adgC7CvhXM8DQ3FVf3s1oDrOhMcChPFXF1ds/8Wrz54Q6xN6v8jzgLtFpLqInAacH6M0vgacJyLdvQbleyj9f+AlYAwug3w1IB07gd0i0ha4Nsw0vAIMF5F2XoAKTH9d3K/zfV59/uV+6zbjqnZahzj228BxInK5iFQVkcuAdsBbYaYtMB1BP2dV/QnXNjDJa8yuJiK+APIMMEJE+opIFRFp5n0+AAuBwd72OcAlYaRhP66Ul44rpfnSUICrrvuniBztlTZO80p7eAGhAPg/rPRQZhYgjL8HgVq4X2efAzMr6LxDcQ29W3H1/i/jMoZgHqSMaVTVJcD1uEz/J2A7sL6U3f6Fa6+Zrapb/Jbfgsu8dwFPeWkOJw3veNcwG1jpPfu7DrhHRHbh2kxe8dt3LzAR+ERc76lTA469FTgP9+t/K67R9ryAdIfrQUr+nK8E8nGlqJ9xbTCo6pe4RvAHgB3A/ygq1YzH/eLfDvyZ4iWyYF7AleB+BJZ66fB3C/ANMA/X5vB3iudpLwAdcG1apgxsoJxJOCLyMrBcVWNegjHJS0R+C1ytqt3jnZbKykoQJu5E5GQR+Y1XJdEfV+88I87JMpWYV313HTA53mmpzCxAmERwFK4L5m5cH/5rVXVBXFNkKi0ROQvXXrOJ0quxTAmsiskYY0xQVoIwxhgTVFJN1teoUSPNzMyMdzKMMabSmD9//hZVbRxsXUwDhNfg+BCQBjytqn8LWF8f1wWthZeW+1X1OW/dGlwXwkPAQVXNKe18mZmZ5OXlRfUajDEmmYlI4Oj7QjELEN6Q/Mdwk5KtB+aJyJve6FSf64Glqnq+iDQGVojINFU94K3vXcY+3MYYY8oplm0QXXATtK32MvxcXPdFfwrU9Wb1rIMb7GJzwBtjTAKIZYBoRvFJydZTfNIwcNMHn4CbQ+YbYKw3hB5c8HhPROaLyNWhTiIiV4tInojkbd68OXqpN8aYFBfLABFs4rLAPrVn4eZnORo3nfCjIlLPW9dNVbNxE6Vd7zfXS/EDqk5W1RxVzWncOGg7izHGmDKIZYBYT/FZK5vjSgr+RgBvqLMSd7/dtgCqusF7/hmYjquyMsYYU0FiGSDmAW3E3U6yOjAYN42xv7W46XgRkSa46YVXi0htEanrLa8NnAksjmFajTGm0pk2DTIzoUoV9zxtWml7RCZmvZhU9aCIjAbexXVzfVZVl4jIKG/9E8BfgCki8g2uSup2Vd0iIq2B6d4dKasCL6lqRc0saowxUTdtGowbB2vXQsOGbtm2bWV/vXUriIBvMowffoCrvdbaoUOjk+akmmojJydHbRyEMSYWIs3gW7SAc86Bt992mbd/Zh5LLVvCmjXhby8i80ONM7MAYYxJGWX9FR/4az2RiUBBQenbFW0fOkAk1VQbxpjUEkmGH5jJb/W7G3k4rytDcABXcokWCxDGmIQQTmZfUrVNMmXyZZWeDhMnRu94FiCMMTERi1/3P/wAjz9e9D7ZM/xw+D63li1dcIhWAzVYgDDGlEFpmX9ZqnNSIbP3fSYZGe59eXox+UpU0Q4K/ixAGGOKiVbmn4wZfiQZvH912Nq1sc/MY8EChDFJzD+z98+gSgoCu3bBAW8+5WTM/Mv6K74yZvDlZd1cjamkIv2lD0XvK0uXzXBE+qs+1TL50lg3V2MqsWCBoKzVPL73iRocSsvsk6HapjKxAGFMgog0ECRqJu/Pft1XbhYgjKlgyRIIwvm1bxl+5WYBwpgoCdUmEO7grkQJBNWqQb169mvfWIAwJmKRlgDiPbgrVMN0sBKAZfzGXyzvB2FMpeY/136jRu4hAlde6TJ9VRcIfMEgHiUA8e7bmJHhHiLFX7dsCVOnurRNnereBy7fssU9CgrcLKAWHIyPlSBMSirPYLB4BYLy/tofOtQyfxMZCxAmZfiCQriTvFXWQGBMtFiAMEktVFCId4OwBQJTGViAMEkhnIbjiggKoTJ+G9xlKiMLEKZSCgwIoeYPilVQsBKASQUWIEylEU4bQrRZIDCpzAKESWixbEOwkcDGlMwChEk4FREUYnH3LWOSjQUIkxAsKBiTeGwktYkb30hl/9HJULagUK1a6SOJbZSwMZGxEoSpUNEsKVjJwJjYsgBhYs6CgjGVkwUIExMWFIyp/CxAmKixoGBMcrEAYcrFgoIxycsChCmzadPg6qth71733oKCMcklpt1cRaS/iKwQkZUickeQ9fVF5D8iskhElojIiHD3NfHj6556xRVFwSESvpvcWBdUYxJbzAKEiKQBjwFnA+2AISLSLmCz64GlqpoF9AL+T0Sqh7mviQNfqcE3ZiFcFhSMqXxiWYLoAqxU1dWqegDIBQYEbKNAXRERoA6wDTgY5r4mDsaNC7/UYEHBmMotlgGiGbDO7/16b5m/R4ETgA3AN8BYVS0Ic18ARORqEckTkbzNmzdHK+0mgK9aqbSSgwUFY5JHLAOEBFkW2Ix5FrAQOBroCDwqIvXC3NctVJ2sqjmqmtO4ceOyp9aEFG61kgUFY5JLLHsxrQeO8XvfHFdS8DcC+JuqKrBSRL4H2oa5r6kgpVUrpafD5MkWEIxJNrEsQcwD2ohIKxGpDgwG3gzYZi3QF0BEmgDHA6vD3NdUkLVrQ69r2dKCgzHJKmYlCFU9KCKjgXeBNOBZVV0iIqO89U8AfwGmiMg3uGql21V1C0CwfWOVVlOyFi2CVy+1bOmqkowxyUm0Iu7kXkFycnI0Ly8v3slIOoED4sCqlYxJFiIyX1Vzgq2z+0GYkHw9l668EmrVKn6PBQsOxiQ/m2rDBBVYati61ZUapk61wGBMqrAShCnkKzFUqQLDhh3ec2nvXtejyRiTGqwEYYDDSwyHDgXfrqQeTcaY5GIliBQX6cR7LVrEPEnGmARhJYgUFqx3UknS092U3MaY1GAliBQWzsR7aWnWc8mYVGUBIgWFO/Feejo8/zwUFNjcSsakIgsQKSaSifesxGBMarM2iBThf+/oktgIaWOMj5UgkpivKknEjYa2UoMxJhJWgkhSgT2USptyyybeM8YEshJEkork1qDWfdUYE4wFiCQTbg8lH6tWMsaEYlVMSSSSgW/WGG2MKY2VIJJIadVK4t3p20oNxphwWIBIAuFUK7Vs6abqVrVBb8aY8FgVUyUXTrWS9VAyxpSFlSAqqXBnYbUeSsaYsrIAUYnYwDdjTEWyKqZKwga+GWMqmpUgKgkb+GaMqWgWIBKcDXwzxsSLVTElMBv4ZoyJJytBJKBweyjZwDdjTCxZCSJB+N+vQSS8RuiJEy0oGGNixwJEHIUKCtZDyRiTCCxAxEmk3VZ9rIeSMaaiWBtEBfK1LVSpAsOGhd9t1cfaGowxFSmmAUJE+ovIChFZKSJ3BFl/q4gs9B6LReSQiDT01q0RkW+8dXmxTGcsBRv9rAqHDoV/jPR0ePFFm2TPGFOxYlbFJCJpwGPAGcB6YJ6IvKmqS33bqOo/gH94258P3KSq2/wO01tVt8QqjbFW1mokKGqTsMZoY0y8xLIE0QVYqaqrVfUAkAsMKGH7IcC/YpieChNuN9VA/t1WbWpuY0y8xTJANAPW+b1f7y07jIikA/2B1/0WK/CeiMwXkatDnURErhaRPBHJ27x5cxSSXTaRTqTnk5bm9rGgYIxJNLHsxSRBloWqZDkf+CSgeqmbqm4QkSOB90VkuarOPeyAqpOByQA5OTkRVOJET3l6JFmjszEmUcWyBLEeOMbvfXNgQ4htBxNQvaSqG7znn4HpuCqrhBTJRHo2+tkYU1nEMkDMA9qISCsRqY4LAm8GbiQi9YHTgX/7LastInV9r4EzgcUxTGu5rF0b3nZWjWSMqUxiFiBU9SAwGngXWAa8oqpLRGSUiIzy23Qg8J6q7vFb1gT4WEQWAV8C/1XVmbFKa1n52h1Kq1KybqrGmMpINJK+lwkuJydH8/IqZshEaTOtWjdVY0xlICLzVTUn2DqbaqOMSmp3sKBgjEkGFiDKKFS7g4hNpGeMSQ6ltkGIyHkiYnM2BWjRIrLlxhhT2YST8Q8GvhOR+0TkhFgnqLKYONE1PvuzmVaNMcmk1AChqlcAnYBVwHMi8pk3erluzFOXgHw9l668EmrVgoyMopHQNq7BGJNMwqo6UtWduGkwcoGmuK6pX4nIDTFMW8Lx9Vzyzci6dSv8+qsb22BdWI0xyabUbq7eLKsjgd8AU4HnVfVnb/6kZaraMvbJDE+su7lmZgafY8nu8GYSUX5+PuvXr2ffvn3xTopJADVr1qR58+ZUq1at2PLydnMdBDwQOA+Squ4VkZFlTm0lFKrnUrgjqY2pSOvXr6du3bpkZmYiEmxqNJMqVJWtW7eyfv16WrVqFfZ+4VQxTcCNZgZARGqJSKZ30g8iTWhlZj2XTGWyb98+MjIyLDgYRISMjIyIS5PhBIhXgQK/94e8ZSnHei6ZysaCg/Epy3chnABR1bvhDwDe6+oRnykJDB3qeiq1bGk9l4wpydatW+nYsSMdO3bkqKOOolmzZoXvDxw4UOK+eXl5jBkzptRzdO3aNVrJNSGE0waxWUQuUNU3AURkAFBpbwNaXkOHWkAwyWnaNDeFzNq1rtq0PNPFZGRksHDhQgDuvvtu6tSpwy233FK4/uDBg1StGjz7ycnJIScnaJtpMZ9++mnZEhdHhw4dIi0tLd7JCFs4JYhRwJ9EZK2IrANuB66JbbKMMRUpsAv3Dz+499OmRe8cw4cP5+abb6Z3797cfvvtfPnll3Tt2pVOnTrRtWtXVqxYAcCHH37IeeedB7jgMnLkSHr16kXr1q15+OGHC49Xp06dwu179erFJZdcQtu2bRk6dCi+3plvv/02bdu2pXv37owZM6bwuP7WrFlDjx49yM7OJjs7u1jgue++++jQoQNZWVnccccdAKxcuZJ+/fqRlZVFdnY2q1atKpZmgNGjRzNlyhQAMjMzueeee+jevTuvvvoqTz31FCeffDJZWVlcfPHF7PUmddu0aRMDBw4kKyuLrKwsPv30U8aPH89DDz1UeNxx48YV+wxirdQShKquAk4VkTq4brG7Yp8sY0xFCjb55N69bnk0S8zffvsts2bNIi0tjZ07dzJ37lyqVq3KrFmz+NOf/sTrr79+2D7Lly9nzpw57Nq1i+OPP55rr732sK6aCxYsYMmSJRx99NF069aNTz75hJycHK655hrmzp1Lq1atGDJkSNA0HXnkkbz//vvUrFmT7777jiFDhpCXl8c777zDjBkz+OKLL0hPT2fbNnfDy6FDh3LHHXcwcOBA9u3bR0FBAevWrQt6bJ+aNWvy8ccfA6767fe//z0Ad955J8888ww33HADY8aM4fTTT2f69OkcOnSI3bt3c/TRR3PRRRcxduxYCgoKyM3N5csvvyzpVFEV1mR9InIu0B6o6WvoUNV7YpiuhBLNorcxiaiiunAPGjSosIplx44dDBs2jO+++w4RIT8/P+g+5557LjVq1KBGjRoceeSRbNq0iebNmxfbpkuXLoXLOnbsyJo1a6hTpw6tW7cu7NY5ZMgQJk+efNjx8/PzGT16NAsXLiQtLY1vv/0WgFmzZjFixAjSvZ4pDRs2ZNeuXfz4448MHDgQcBl/OC677LLC14sXL+bOO+/kl19+Yffu3Zx11lkAzJ49mxdeeAGAtLQ06tevT/369cnIyGDBggVs2rSJTp06kZGREdY5o6HUACEiTwDpQG/gaeAS/Lq9JrvA+z74it5gQcIkjxYtgg8CjXYX7tq1axe+Hj9+PL1792b69OmsWbOGXr16Bd2nRo0aha/T0tI4ePBgWNuEe6+bBx54gCZNmrBo0SIKCgoKM31VPaznT6hjVq1alYKCos6egd1J/a97+PDhzJgxg6ysLKZMmcKHH35YYvp+97vfMWXKFDZu3MjIkRU79CycNoiuqvpbYLuq/hk4jeL3mk5qJRW9jUkW8ejCvWPHDpo1awZQWF8fTW3btmX16tWs8aY5ePnll0Omo2nTplSpUoWpU6dy6NAhAM4880yeffbZwjaCbdu2Ua9ePZo3b86MGTMA2L9/P3v37qVly5YsXbqU/fv3s2PHDj74IPQQsV27dtG0aVPy8/OZ5tfI07dvXx5//HHANWbv3LkTgIEDBzJz5kzmzZtXWNqoKOEECF8o3CsiRwP5QPhD8So5Gz1tUkE8unDfdttt/PGPf6Rbt26FmXI01apVi0mTJtG/f3+6d+9OkyZNqF+//mHbXXfddTz//POceuqpfPvtt4W/9vv3788FF1xATk4OHTt25P777wdg6tSpPPzww5x00kl07dqVjRs3cswxx3DppZdy0kknMXToUDp16hQyXX/5y1845ZRTOOOMM2jbtm3h8oceeog5c+bQoUMHOnfuzJIlSwCoXr06vXv35tJLL63wHlDhzMU0HngE6As8BijwlKreFfvkRSYWczHZ/Eumslq2bBknnJDaM/Tv3r2bOnXqoKpcf/31tGnThptuuineyYpIQUEB2dnZvPrqq7Rp06Zcxwr2nShpLqYSSxDejYI+UNVfVPV1oCXQNhGDQ6zY6GljKq+nnnqKjh070r59e3bs2ME111SuHvpLly7l2GOPpW/fvuUODmURTgniM1U9rYLSUy7RLEH491xq2NAt27bNejGZysNKECZQpCWIcLq5viciFwNvaLjdAiq5wJ5LW7e6UsPUqRYYjDGpI5xG6ptxk/PtF5GdIrJLRHbGOF1xZT2XjDEmvJHUKXdrUeu5ZIwx4Q2U6xlseeANhJJJRQ0aMsaYRBZOFdOtfo/xwH+Au2OYpriznkvGlE+vXr149913iy178MEHue6660rcx9fJ5JxzzuGXX345bJu77767cDxCKDNmzGDp0qWF7++66y5mzZoVQeqNT6kBQlXP93ucAZwIbIp90uLH7vtgTPkMGTKE3NzcYstyc3NDTpgX6O2336ZBgwZlOndggLjnnnvo169fmY4VL7EYOFgW4ZQgAq3HBYmkMG2aGwxXpQo0auQeVaq4BumJE6GgwA2Is+BgTPguueQS3nrrLfbv3w+4KbU3bNhA9+7dufbaa8nJyaF9+/ZMmDAh6P6ZmZls2eJuOzNx4kSOP/54+vXrVzglOBB02uxPP/2UN998k1tvvZWOHTuyatUqhg8fzmuvvQbABx98QKdOnejQoQMjR44sTF9mZiYTJkwgOzubDh06sHz58sPSlIrTgofTBvEIbvQ0uIDSEVhU7jMngGDdWX1sUj6TTG68Ebz790RNx47w4IPB12VkZNClSxdmzpzJgAEDyM3N5bLLLkNEmDhxIg0bNuTQoUP07duXr7/+mpNOOinocebPn09ubi4LFizg4MGDZGdn07lzZwAuuuiioNNmX3DBBZx33nlccsklxY61b98+hg8fzgcffMBxxx3Hb3/7Wx5//HFuvPFGABo1asRXX33FpEmTuP/++3n66aeL7Z+K04KHU4LIA+Z7j8+A21X1inKfOQEE687qz7q2GlN2/tVM/tVLr7zyCtnZ2XTq1IklS5YUqw4K9NFHHzFw4EDS09OpV68eF1xwQeG6xYsX06NHDzp06MC0adMK5y4KZcWKFbRq1YrjjjsOgGHDhjF3blFfm4suugiAzp07F07w5y8/P5/f//73dOjQgUGDBhWmO9xpwdMDGzaDCJwWPNj1zZ49m2uvvRYomhY8MzOzcFrw9957L2rTgoczUO41YJ+qHgIQkTQRSVfVErJWR0T6Aw8BacDTqvq3gPW3Ar7f51WBE4DGqrqttH2jIZxuq9a11SSDUL/0Y+nCCy/k5ptv5quvvuLXX38lOzub77//nvvvv5958+ZxxBFHMHz48MOmxg4UOOW2T6TTZpc2ztc3ZXioKcVTcVrwcEoQHwC1/N7XAkrtEiAiabjJ/c4G2gFDRKSd/zaq+g9V7aiqHYE/Av/zgkOp+0ZDON1WrWurMWVTp04devXqxciRIwtLDzt37qR27drUr1+fTZs28c4775R4jJ49ezJ9+nR+/fVXdu3axX/+85/CdaGmza5bty67dh1+48u2bduyZs0aVq5cCbhZWU8//fSwrycVpwUPJ0DUVNXdvjfe69LLStAFWKmqq1X1AJALDChh+yHAv8q4b5lMnAi1aoVeb11bjSmfIUOGsGjRIgYPHgxAVlYWnTp1on379owcOZJu3bqVuH92djaXXXYZHTt25OKLL6ZHjx6F60JNmz148GD+8Y9/0KlTJ1atWlW4vGbNmjz33HMMGjSIDh06UKVKFUaNGhX2taTktOCqWuID+ATI9nvfGfgsjP0uwVUN+d5fCTwaYtt0YBvQsAz7Xo1rJ8lr0aKFRurFF1VbtlQVUc3IUE1PVwXVo49264yprJYuXRrvJJgKdOjQIc3KytJvv/025DbBvhNAnobIx8MpQdwIvCoiH4nIR8DLwOgw9gtWcRiqEvB84BNV3Rbpvqo6WVVzVDWncePGYSSruKFDXTfWggLYsgUWLHDLo32zdmOMiZVYTQsezlxM80SkLXA8LuNerqrB7y5e3HqK35q0ObAhxLaDKapeinTfqGrTBlq3hpkzoYRBn8YYkzDatWvH6tWro37cUksQInI9UFtVF6vqN0AdEQkn65wHtBGRViJSHRcE3gxy/PrA6cC/I903FkSgf3+YPRu8MTTGGJOSwqli+r2q/uJ7o6rbgd+XtpOqHsRVRb0LLANeUdUlIjJKRPxbhgYC76nqntL2DSOtUdG/P+zZA954FWMqLU2NW7iYMJTluxDOOIgqIiJeY4av+2r1MBP0NvB2wLInAt5PAaaEs29F6d0bqleHd96Bvn3jkQJjyq9mzZps3bqVjIyMkGMJTGpQVbZu3Vo4diNc4QSId4FXROQJXEPxKKDkzsuVXJ060KOHa4coZeJIYxJW8+bNWb9+PZs3b453UkwCqFmzJs2bN49on3ACxO24rqTX4hqpFwBNI05dJXP22XDLLbBuHRxzTOnbG5NoqlWrRqtWreKdDFOJhTPddwHwObAayAH64toFklr//u555sz4psMYY+IlZIAQkeNE5C4RWQY8CqwDUNXeqvpoRSUwXtq1cyUHCxDGmFRVUgliOa60cL6qdlfVR4DEuItFBRCBfv1gzhw3iM4YY1JNSQHiYmAjMEdEnhKRvgQf4Zy0+vSB7dthUVLc/cIYYyITMkCo6nRVvQxoC3wI3AQ0EZHHReTMCkpfXPXu7Z5nz45vOowxJh7CaaTeo6rTVPU83JQXC4E7Yp2wRNCsGRx/vAUIY0xqiuie1Kq6TVWfVNU+sUpQounTB+bOhfxwZp8yxpgkElGASEV9+sDu3TB/frxTYowxFcsCRCl69XLPVs1kjEk1FiBK0agRnHSSBQhjTOqxABGGPn3gk0+glHurG2NMUrEAEYY+fVxw+PzzeKfEGGMqjgWIMPTsCVWqWDWTMSa1WIAIQ/36kJPjpt0wxphUYQEiTD16wLx5dhtSY0zqsAARpu7dXXCw8RDGmFRhASJMXbu6Z7tPtTEmVViACNORR8Jxx7nursYYkwosQESgWzcXIOz+EMaYVGABIgLdu8PWrbBiRbxTYowxsWcBIgLdurlnq2YyxqQCCxAROO44NzeTNVQbY1KBBYgIiBS1QxhjTLKzABGh7t1h5UrYtCneKTHGmNiyABEha4cwxqQKCxARys6GmjWtHcIYk/wsQESoRg04+WQLEMaY5BfTACEi/UVkhYisFJE7QmzTS0QWisgSEfmf3/I1IvKNty4vlumMVI8esGCBu1e1McYkq5gFCBFJAx4DzgbaAUNEpF3ANg2AScAFqtoeGBRwmN6q2lFVc2KVzrLo2RMOHrQbCBljklssSxBdgJWqulpVDwC5wICAbS4H3lDVtQCq+nMM0xM1Xbu6GwjNnRvvlBhjTOzEMkA0A9b5vV/vLfN3HHCEiHwoIvNF5Ld+6xR4z1t+daiTiMjVIpInInmbN2+OWuJLUreua6y2AGGMSWaxDBASZJkGvK8KdAbOBc4CxovIcd66bqqajauiul5EegY7iapOVtUcVc1p3LhxlJJeuh49XBWT3UDIGJOsYhkg1gPH+L1vDmwIss1MVd2jqluAuUAWgKpu8J5/BqbjqqwSRs+eLjjkJVTzuTHGRE8sA8Q8oI2ItBKR6sBg4M2Abf4N9BCRqiKSDpwCLBOR2iJSF0BEagNnAotjmNaIde/unq2ayRiTrGIWIFT1IDAaeBdYBryiqktEZJSIjPK2WQbMBL4GvgSeVtXFQBPgYxFZ5C3/r6rOjFVay6JRI2jf3gKEMSZ5iWpgs0DllZOTo3kVWOdz3XXw4ouwbRtUrVphpzXGmKgRkfmhhhLYSOpy6NkTdu2CRYvinRJjjIk+CxDl0KOHe/7oo/imwxhjYsECRDk0awatW1s7hDEmOVmAKKfTT4f//c9NvWGMMcnEAkQ5nXOOa6T+9NN4p8QYY6LLAkQ5nXWWmwJ8xox4p8QYY6LLAkQ51a0LffvCv/8NSdRj2BhjLEBEw4UXwurVsDihxnobY0z5WICIgvPPBxFXijDGmGRhASIKjjoKTj3V2iGMMcnFAkSUDBgA8+fDunWlb2uMMZWBBYgoufBC9/xm4Hy1xhhTSVmAiJLjj3cPa4cwxiQLCxBRdMklMGsWTJoU75QYY0z52STVUTRuHHzzDVx/PWzZAuPHu95NxhhTGVkJIopq1YLXX4dhw2DCBLjxRhs8Z4ypvKwEEWVVq8Kzz0KDBvDQQ+7WpIMGxTtVxhgTOStBxECVKnD//dCpE4wdCzt2xDtFxhgTOQsQMVK1KkyeDJs2wZ13xjs1xhgTOQsQMZST4xqsH3sM5s2Ld2oqliqMGeMa7vfurdhzFxTAu+/C7t0Ve95kkiqf3fTprkp4z554pyQxWYCIsXvvhaZN4Zpr4NCheKem4kyaBI88An/9K5x4osuwAQ4cgLVrY1ft9v330Ls39O8PXbrAihVF6776CgYOhJtuCn/E+7Zt8NNPsUlrIlKFiROhXj1XPZrMN8J67DG46CK46ip3d8ixY+Hzz0MHi127XLvi4MHuh8/UqfDZZ7B8ufuO7N9/+D5798K//uW+l9Fw8CAsWeLOPW4cXHmlu2lZ797ROX4g0STqZpOTk6N5eXnxTsZhnn8ehg+Hjz+Gbt3inZroUoWXXoLOnaFtW7ds+XLIznZf3DvucMFxxQrIyICtW902NWrABRe4Hl9ZWe5L/803rmpuxAioX7/08370Ebz6KjRsCC1auH/g8eNdG9BNN7kMYP9+ePxx948/aZLrPLBjh9vmiitcOnfudI8+feDMM4vOsXq1u4b8fHdNDRqU/nkcPOjSccQRZfk042vvXhg5El5+2bWfLVgAZ5zh3od7Pfn58Oc/uwyzVy+XcTVvHnlaVq+G996DNWvg6qvdrX3Lat06ePhh9z258kqXnocecr0Mzz8fbr4ZnnrKfZfy813X9NatoX17+M1v3GPdOnjySfjlF7f/Tz8d/oOvRg247Tb44x9dj8bly10HlcWL3fftkktcEKpd2/1IWrfO/WDyyc52+UNaWtGyffvgiy9gzhz3mDcPfv3VrUtLc2lp0QLatIFnninb5yMi81U1J+hKVU2aR+fOnTURbdmiKqL6l7/EOyXR99hjqqBao4bqAw+o7tunmpOjmpGhumGD22bfPtX77lMdNUr1z39WffJJ1RtucNu4rL74o3591bvuUl2wQPWRR1TPO0+1bVvVK65QfeIJ1VdeUe3a1W1bq5b7bH379u6tumaNO+/ataonn+yWV6miOnq06vbtbv3o0ao1axbtV6WKex43TvXgQbdNixaqDRq4dddfX/y6f/xRddIkd8333ad6662qPXuqpqe79AwapLpoUeSf55497vz+CgpUV65U3bw58uOFa/Fi1c6dXdr/9jd3zmeeUa1WTbVNG9UHH1SdO1d1507VAwfc57hhQ/G0bt+u2rdv0d/Q99mecorqO++4YwYqKFB9/33Vm25yf9/+/VVbty7aV8Sl4aabVLduLb5vfr7qpk2qS5aofvml6ocfuvN88onqihWq33+vevPN7rtZtWrR8XzfiYsuUt2/v+h4P/+sOn266j33uL/fiSe675fv+zFokOrnn7tt9+9XXbZM9b//VX3pJdXHH1cdPNht26qV6oQJ7rvQuLFqbq7q7bcX/0xCPZo2dd+1G25Q7dJFtXr1ovN37qw6dqzqCy+4v1d+fnT+9kCehshT456pR/ORqAFCVbVjR5d5JZMvvnD/vGedpXr++e7b1Ly5e3799dL337/f/UM+8ojq7NkuA5w/X3XgwOL/NL/5jeq556o2aVK0rGVL1UcfVd2712VYq1e7fQ8dKn6OX39V/ec/3bpAO3eqbtzotvn1V9WrrnLH7tfP/ZM3aOD2u+EGl7HMm+f28wUP/zRWr+7+oceMUb3lFtW6dd3y885T/cc/VN9912UoU6eqjhihmpXltv36a3fMFSvc+atVc+cdMMCle+xY1WOPdcdKS3MZ8JNPqr79turDD7v1t9yi+uqrLiBu2eLWTZigOnGi+3xKsnmz6nXXuWM3aKD65pvF13/8cfEMO/Bx1FEuQ/v3v1Xbt3cZ8XPPub/DggXu2lu2dNt27equ/9NPVdevd+k87bSiQJ+Z6X5cXHihu7YVK1wgvuoql0HWqKHasKF7hJPZ+jLWESPc32zVKve5HHusC0YHDpT+HS0ocIHwp59K31bVfY/btnXnPv10l36fnTtd5v7qq+5/Z8MGF1S3b3fBKTfXBa2aNV1w6dlT9bbb3Ge7fXt45y+LkgKEVTFVkD/8wVV5/PIL1KwZ79SU35YtrkicluZmsT3iCHjuOVdsv/RSePrp8h3/m2/ccXv0cEV8cP/yq1bBDz9Az55QrVq5L+MwTz0Fo0e7v9GsWXDyya5Kqm1bV0/92muuKmr7dnjrLWjXzqWjZk1XPeazfbur1njqKfjxx+LnaNgQOnRw9dcHDrhjr1jhqiiGDXPVVHPmuGqWGjXc+c4+GzZudNUg331XdKzatV21iH9VBbhqElWXvmnToGNH154yZUpRA/7eve7z3LsXRo2Cu++GRo2Cfy4//eSqnBYtcp0A0tPd9X74Ibz9tqsKqV8f3njDpdffgQOuIfjeew//LFq0cNWQI0aU/H+xeLH7fvmus0oV9zk2auSe69Z1aapVy1UXbtni/gZ9+8IJJ4Q+biwcOACffOK+u1XLMNJs/373f1WWfcuipComCxAV5K23XH3n7Nmxa1CKpR9+cL2y6tVzGdrGje6f9tNPXfuDz759LlOrzFOMLF/u/kHbtClalpsLQ4a4jEikKHiEY+tWF/C+/97V7Z90ksvgtmxxjY0zZri657FjoUmTov02bHDtHunpRctU3ee+c6cLnE2auAxp0SJXV713r2ucz8lxAWjYMBcYzj7bBYZ9+1xwOvJId9zGjV0dfPv2Zf+8du92Aa1DB8jMDL3dgQMuuK1d675Pdeu6Ovrq1ct+blN+1gaRAHbscMX4O++Md0rKZvz4orr1rCzVI49UnTIl3qmqOAUFriqtTh1XRVJZbN7sqi0aNFC95hrVhQvjnSKTaLAqpsRwyimuOuLjj+OdksgcOgStWrlfme+8E+/UxM++fa6HUuPG8U6JMdFTUgnCxkFUoD59XDVAZRuENHu265I3YkS8UxJfNWtacDCpxQJEBerd2zVA+koQ+fmuUS/RR3E++6xrCBwwIN4pMcZUpJgGCBHpLyIrRGSliNwRYpteIrJQRJaIyP8i2bey6dbNVTHNmeMGuwwcCBdfDKedBitXxjt1wW3f7qYjuPxy1/hsjEkdMQsQIpIGPAacDbQDhohIu4BtGgCTgAtUtT0wKNx9K6PatV07xDvvwDnnuO6BN97ouv7l5LieTokmN9d1u0v16iVjUlEsSxBdgJWqulpVDwC5QGAlxeXAG6q6FkBVf45g30qpTx/X5fGjj1wXxwcecP39W7d23WBvuMF1YUwUzz7rpsLo1CneKTHGVLRYBohmgP+UaOu9Zf6OA44QkQ9FZL6I/DaCfQEQkatFJE9E8jZv3hylpMfOoEGu//rrr8PQoW5ZZqYbWDNmjBtMd8IJrm2ivB3MDh6EF16AW25xGf28eeHPrPrzz24AU16eKz1U5nENxpiyieVYvWBZSmCWVxXoDPQFagGficjnYe7rFqpOBiaD6+Za5tRWkBNPDN7eUKuWm0Bs6FA3OdnFF7tRr8OGFd9u1Cg3CdrgwaHPoeoGX40bB8uWuRGZvlk5ReDYY92gpl693HTkVfx+JuzY4e5f8fTTrmpp0CA326UxJvXEsgSxHjjG731zYEOQbWaq6h5V3QLMBbLC3DcpdenifrW3besChL9ly9yMkhMnht4/P99NYXzRRW5KhNdecw3i337rSi0TJriRvF9/7Uost9xSVFLZswfOPdfNfnr55e58r7wCderE7HKNMYks1Ai68j5wpYPVQCugOrAIaB+wzQnAB9626cBi4MRw9g32SOSR1JG66y43ctl/krAJE4omIfvmm8P3OXRIdehQt/7vfy95tseCAjdZHKjee6+bcfWMM9zkZq+8EvXLMcYkKEoYSR2zEoSqHgRGA+8Cy4BXVHWJiIwSkVHeNsuAmcDXwJfA06q6ONS+sUprIho0yIWCN95w71XdvPy+eXxefrn49qquR9S0aa6EcdttJU/2JeIayK+4wlUpdekC77/vqpYGDYrZZRljKhGbaiNB+WbiPOooN25i0SI3I+cTT7hqo++/dxOf+RqP//pX1+Zw881w//3hNyrn57v2jv/8Bx580E0YZ4xJHTbVRiUk4qbNnjvXzZz68stuhtGLL3YN1KtWue6x4G6ledddbrbRSIIDuIF7r73mApAFB2OMPwsQCWzQINfQ/PrrbsBav35u/vuBA13GnpvrSgBXXeXmCHrssbJ1R61e3VVdGWOMvwq6JYUpi/btXW+mv/7V3Rtg/Hi3vGFDOOssV6po1AgWLnRBpDLeB9kYk7isBJHARFwpYsMGV2IYOLBo3eDBsH49/OlPRd1ajTEmmixAJLhLL3XP/fu7u4v5XHCBm366fn149NG4JM0Yk+SsiinBtW8P99zjJvfzV7eu65LapAk0bRqftBljkpt1czXGmBRm3VyNMcZEzAKEMcaYoCxAGGOMCcoChDHGmKAsQBhjjAnKAoQxxpigLEAYY4wJygKEMcaYoJJqoJyIbAZ+iGCXRsCWGCUnUaXiNUNqXncqXjOk5nWX55pbqmrjYCuSKkBESkTyQo0gTFapeM2QmteditcMqXndsbpmq2IyxhgTlAUIY4wxQaV6gJgc7wTEQSpeM6TmdafiNUNqXndMrjml2yCMMcaEluolCGOMMSFYgDDGGBNUSgYIEekvIitEZKWI3BHv9MSKiBwjInNEZJmILBGRsd7yhiLyvoh85z0fEe+0RpuIpInIAhF5y3ufCtfcQEReE5Hl3t/8tGS/bhG5yftuLxaRf4lIzWS8ZhF5VkR+FpHFfstCXqeI/NHL31aIyFllPW/KBQgRSQMeA84G2gFDRKRdfFMVMweBP6jqCcCpwPXetd4BfKCqbYAPvPfJZiywzO99KlzzQ8BMVW0LZOGuP2mvW0SaAWOAHFU9EUgDBpOc1zwF6B+wLOh1ev/jg4H23j6TvHwvYikXIIAuwEpVXa2qB4BcYECc0xQTqvqTqn7lvd6FyzCa4a73eW+z54EL45LAGBGR5sC5wNN+i5P9musBPYFnAFT1gKr+QpJfN1AVqCUiVYF0YANJeM2qOhfYFrA41HUOAHJVdb+qfg+sxOV7EUvFANEMWOf3fr23LKmJSCbQCfgCaKKqP4ELIsCRcUxaLDwI3AYU+C1L9mtuDWwGnvOq1p4Wkdok8XWr6o/A/cBa4Cdgh6q+RxJfc4BQ1xm1PC4VA4QEWZbUfX1FpA7wOnCjqu6Md3piSUTOA35W1fnxTksFqwpkA4+raidgD8lRtRKSV+c+AGgFHA3UFpEr4puqhBC1PC4VA8R64Bi/981xxdKkJCLVcMFhmqq+4S3eJCJNvfVNgZ/jlb4Y6AZcICJrcNWHfUTkRZL7msF9r9er6hfe+9dwASOZr7sf8L2qblbVfOANoCvJfc3+Ql1n1PK4VAwQ84A2ItJKRKrjGnPejHOaYkJEBFcnvUxV/+m36k1gmPd6GPDvik5brKjqH1W1uapm4v62s1X1CpL4mgFUdSOwTkSO9xb1BZaS3Ne9FjhVRNK973pfXDtbMl+zv1DX+SYwWERqiEgroA3wZZnOoKop9wDOAb4FVgHj4p2eGF5nd1zR8mtgofc4B8jA9Xr4zntuGO+0xuj6ewFvea+T/pqBjkCe9/eeARyR7NcN/BlYDiwGpgI1kvGagX/h2lnycSWEq0q6TmCcl7+tAM4u63ltqg1jjDFBpWIVkzHGmDBYgDDGGBOUBQhjjDFBWYAwxhgTlAUIY4wxQVmAMKYUInJIRBb6PaI2QllEMv1n6DQmkVSNdwKMqQR+VdWO8U6EMRXNShDGlJGIrBGRv4vIl97jWG95SxH5QES+9p5beMubiMh0EVnkPbp6h0oTkae8+xq8JyK1vO3HiMhS7zi5cbpMk8IsQBhTuloBVUyX+a3bqapdgEdxs8jivX5BVU8CpgEPe8sfBv6nqlm4eZKWeMvbAI+panvgF+Bib/kdQCfvOKNic2nGhGYjqY0phYjsVtU6QZavAfqo6mpvUsSNqpohIluApqqa7y3/SVUbichmoLmq7vc7RibwvrqbviAitwPVVPVeEZkJ7MZNmzFDVXfH+FKNKcZKEMaUj4Z4HWqbYPb7vT5EUdvgubi7H3YG5ns3xTGmwliAMKZ8LvN7/sx7/SluJlmAocDH3usPgGuh8J7Z9UIdVESqAMeo6hzczY8aAIeVYoyJJftFYkzpaonIQr/3M1XV19W1hoh8gfuxNcRbNgZ4VkRuxd3lbYS3fCwwWUSuwpUUrsXN0BlMGvCiiNTH3QDmAXW3EDWmwlgbhDFl5LVB5KjqlninxZhYsComY4wxQVkJwhhjTFBWgjDGGBOUBQhjjDFBWYAwxhgTlAUIY4wxQVmAMMYYE9T/A/1vOnApA2WcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation accuracy with 7 hidden layers\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training and validation accuracy with 1 hidden layer\")\n",
    "plot_history(history1HL)\n",
    "print(\"Training and validation accuracy with 5 hidden layers\")\n",
    "plot_history(history5HL)\n",
    "print(\"Training and validation accuracy with 7 hidden layers\")\n",
    "plot_history(history7HL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of model creation and hiden layers\n",
    "As we can see above, increasing the number of hidden layers will increase the training accuracy of the model but the validation accuracy appears to be unrelated to the hidden layer size. An increased number of epochs may allow an even greater level of training accuracy and a greater number of hidden layers may also lead to a greater training accuracy. However, as my computer would not be able to handle an increased number of epochs or more hidden layers, we will continue with only 7 hidden layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN_Sumariser\n",
    "The following function operates in a similar way to the cosine summariser of question 2 with the following steps:\n",
    "<ol>\n",
    "    <li> Loops through each given question id. </li>\n",
    "    <li> Stores the output of the question run through the model </li>\n",
    "    <li> Loops through each answer to the current question </li>\n",
    "    <li> Finds the prediction value from the model of the current answer to the current question </li>\n",
    "    <li> Finds the dot product of the answer prediction value and the question prediction value </li>\n",
    "    <li> Adds the dot product to the results list if it is better than at least one of the current elements </li>\n",
    "    <li> Returns a sorted list (by dot product score) of answer ids for the question </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_summariser(csvfile, questionids, n=5):\n",
    " \n",
    "         totalResults = []\n",
    "         totalResults_scores = []\n",
    "         file = pd.read_csv(csvfile)\n",
    "         \n",
    "         for qid in questionids:\n",
    "            if(len(file[file.qid == qid]) > 0): \n",
    "               results = []\n",
    "               results_scores = []\n",
    "               results_worst = 1\n",
    "               \n",
    "               df = file[file['qid'] == qid].copy()\n",
    "               \n",
    "               for i in range(0,len(df.index)):\n",
    "                  predictValue = vectorizer.transform([df['sentence text'].iloc(0)[i]]).toarray()\n",
    "                  answerID = df['sentid'].iloc(0)[i]\n",
    "                  score = model7HL.predict(predictValue)\n",
    "                  if len(results) < n:\n",
    "                        # It is one of the top n results by default\n",
    "                     results.append(answerID)\n",
    "                     results_scores.append(score)\n",
    "                     results_worst = np.min((results_worst, score))\n",
    "                     continue\n",
    "                  if score > results_worst:\n",
    "                     # It is one of the top n results; replace with worst so far\n",
    "                     j = np.argmin(results_scores)\n",
    "                     results_scores[j] = score\n",
    "                     results[j] = answerID\n",
    "                     results_worst = np.min(results_scores)\n",
    "               results = sorted(results, key=lambda x: results_scores[results.index(x)], reverse=True)\n",
    "               totalResults.append(results)\n",
    "               totalResults_scores.append(results_scores)\n",
    "      \n",
    "         return sorted(totalResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the nn_sumariser \n",
    "Again similar to question 2, we will run the nn_sumariser for each question in the test set and store the f1_score of the comparison between the actual and predicted values of each answer. From this array we will find the average f1Score which will give us a good understanding of how accurate the model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nnf1Scores = []\n",
    "for qid in test_set['qid'].unique():\n",
    "    actual = test_set[test_set.qid == qid].sort_values(by=['label'])['sentid'].to_numpy()[:n]\n",
    "    predicted = nn_summariser('test.csv', [qid], n)\n",
    "    predicted = np.array(predicted[0])\n",
    "    nnf1Scores.append(f1_score(actual, predicted, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1711727666273121"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(nnf1Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of evaluation\n",
    "As seen above, the mean of the f1 scores of my model was 0.17 which is means that my model was inaccurate. As the cosine summariser had an accuracy of 0.2, the simple nn_summariser is less accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0NeK3gM0dC9"
   },
   "source": [
    "# Task 4 (5 marks): Recurrent NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a more complex neural network that is composed of the following layers:\n",
    "\n",
    "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
    "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
    "* The final output layer with one cell for binary classification, as in task 3.\n",
    "\n",
    "Train the model with the training data, use the dev_test set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
    "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words and max sentence length\n",
    "We will use the previous tf.idf vectorizer in order to find the total number of words to consider. <br>\n",
    "We will also graph a historgram of all the sentence lengths in the training data. From this we can see what the length of the majority of sentences is and will be able to chose an appropriate value for the maximum sentence length to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['sentence text'].str.len().values\n",
    "sentence_lengths = training['sentence text'].str.len().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.279e+04, 5.559e+03, 2.200e+02, 2.600e+01, 2.100e+01, 1.600e+01,\n",
       "        9.000e+00, 1.000e+01, 1.000e+00, 5.000e+00]),\n",
       " array([1.0000e+00, 2.3480e+02, 4.6860e+02, 7.0240e+02, 9.3620e+02,\n",
       "        1.1700e+03, 1.4038e+03, 1.6376e+03, 1.8714e+03, 2.1052e+03,\n",
       "        2.3390e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sentence_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in this graph, there are very few sentences that have a length above 500. So if we set the maximum length to be 500, the majority of sentences should be covered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining amount of words to consider\n",
    "numWords = len(vectorizer.get_feature_names())\n",
    "# Determining the maximum string length to consider\n",
    "maxlen = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training and DevTest Vectors\n",
    "We will use the tokenizer and pad_sequences in order to prepare our data to train the model. <br>\n",
    "The tokenier first creates a dictionary of all words in the training set and assigns them various numbers representing each word. <br>\n",
    "The pad_sequences function then ensures the vectors for devtest and training values are the same length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(numWords)\n",
    "train_texts = training['sentence text'].to_numpy()\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "training_vectors = pad_sequences(training_sequences, maxlen=maxlen)\n",
    "devtest_sequences = tokenizer.texts_to_sequences(devtest_set['sentence text'].to_numpy())\n",
    "devtest_vectors = pad_sequences(devtest_sequences, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38657, 500)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the LSTM Model\n",
    "We will create the LSTM model using an embedding layer, a LSTM layer of an equal size to the embedding layer, a dense hidden layer with a sigmoid activiation and a binary loss function, ensuring a binary output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "c8RRCWeQTrPl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, None, 35)          350000    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 35)                9940      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 359,976\n",
      "Trainable params: 359,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "lengths = training[\"sentence text\"].str.len()\n",
    "\n",
    "lstm_model = models.Sequential()\n",
    "lstm_model.add(Embedding(numWords, 35))\n",
    "lstm_model.add(LSTM(35))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "We will fit the model with only 5 epochs as my computer is a bit alow however I would ideally fit the model with 10 epochs. We will also use the devtest set as validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "967/967 [==============================] - 187s 193ms/step - loss: 0.5750 - acc: 0.7104 - val_loss: 0.6018 - val_acc: 0.6936\n",
      "Epoch 2/5\n",
      "967/967 [==============================] - 192s 199ms/step - loss: 0.4712 - acc: 0.7752 - val_loss: 0.6810 - val_acc: 0.6170\n",
      "Epoch 3/5\n",
      "967/967 [==============================] - 202s 209ms/step - loss: 0.3907 - acc: 0.8252 - val_loss: 0.7283 - val_acc: 0.6001\n",
      "Epoch 4/5\n",
      "967/967 [==============================] - 189s 196ms/step - loss: 0.3233 - acc: 0.8623 - val_loss: 0.8282 - val_acc: 0.6350\n",
      "Epoch 5/5\n",
      "967/967 [==============================] - 199s 206ms/step - loss: 0.2595 - acc: 0.8925 - val_loss: 0.9796 - val_acc: 0.6011\n"
     ]
    }
   ],
   "source": [
    "history = lstm_model.fit(training_vectors, \n",
    "                    np.array(training['label']),\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_split = 0.2,\n",
    "                    validation_data=(devtest_vectors, np.array(devtest_set['label'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Summariser\n",
    "In the same way as questions 1 and 2, the following function takes in a question ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_summariser(csvfile, questionids, n=5):\n",
    " \n",
    "         totalResults = []\n",
    "         totalResults_scores = []\n",
    "         file = pd.read_csv(csvfile)\n",
    "         \n",
    "         for qid in questionids:\n",
    "            if(len(file[file.qid == qid]) > 0): \n",
    "               results = []\n",
    "               results_scores = []\n",
    "               results_worst = 1\n",
    "               \n",
    "               df = file[file['qid'] == qid].copy()\n",
    "               \n",
    "               for i in range(0,len(df.index)):\n",
    "                  predictValue = tokenizer.texts_to_sequences([df['sentence text'].iloc(0)[i]])\n",
    "                  answerID = df['sentid'].iloc(0)[i]\n",
    "                  if(len(predictValue[0]) < 1):\n",
    "                     continue\n",
    "                  score = lstm_model.predict(predictValue)\n",
    "                  if len(results) < n:\n",
    "                        # It is one of the top n results by default\n",
    "                     results.append(answerID)\n",
    "                     results_scores.append(score)\n",
    "                     results_worst = np.min((results_worst, score))\n",
    "                     continue\n",
    "                  if score > results_worst:\n",
    "                     # It is one of the top n results; replace with worst so far\n",
    "                     j = np.argmin(results_scores)\n",
    "                     results_scores[j] = score\n",
    "                     results[j] = answerID\n",
    "                     results_worst = np.min(results_scores)\n",
    "               results = sorted(results, key=lambda x: results_scores[results.index(x)], reverse=True)\n",
    "               totalResults.append(results)\n",
    "               totalResults_scores.append(results_scores)\n",
    "      \n",
    "         return sorted(totalResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "lstmf1Scores = []\n",
    "for qid in test_set['qid'].unique():\n",
    "    actual = test_set[test_set.qid == qid].sort_values(by=['label'])['sentid'].to_numpy()[:n]\n",
    "    predicted = lstm_summariser('test.csv', [qid], n)\n",
    "    predicted = np.array(predicted[0])\n",
    "    if(len(actual) == len(predicted)):\n",
    "        lstmf1Scores.append(f1_score(actual, predicted, average='micro'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16118343195266274"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lstmf1Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the  LSTM Summariser \n",
    "As seen above, the mean value of the f1Score of the LSTM summariser is 0.16 which is worse than both 0.2 of the cosine summariser and 0.17 of the the NN sumariser. Potentially with a larger amount of epochs or a different way of comapring the labels to the output of the model the accuracy could be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppkBsuB_0dC9"
   },
   "source": [
    "# Submission of results\n",
    "\n",
    "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
    "\n",
    "**Do not submit multiple files. If you feel you need to submit multiple files, please contact Diego.Molla-Aliod@mq.edu.au first.**\n",
    "\n",
    "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells.\n",
    "\n",
    "Each task specifies a number of marks. The final mark of the assignment is the sum of all the marks of each individual task.\n",
    "\n",
    "By submitting this assignment you are acknowledging that this is your own work. Any submissions that break the code of academic honesty will be penalised as per [the academic integrity policy](https://policies.mq.edu.au/document/view.php?id=3).\n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "0d4eb222e854d6dcf0ad3595ad06f3f990a60f1352b3f283c2ce0fb8668d5aef"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
