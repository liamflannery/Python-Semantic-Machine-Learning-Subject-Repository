{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Statistics and NLTK\n",
    "\n",
    "The following exercises use a portion of the Gutenberg corpus that is stored in the corpus dataset of NLTK. [The Project Gutenberg](http://www.gutenberg.org/) is a large collection of electronic books that are out of copyright. These books are free to download for reading, or for our case, for doing a little of corpus analysis.\n",
    "\n",
    "To obtain the list of files of NLTK's Gutenberg corpus, type the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/diego/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain all words in the entire Gutenberg corpus of NLTK, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenbergwords = nltk.corpus.gutenberg.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find the total number of words, and the first 10 words (do not attempt to display all the words or your computer will freeze!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenbergwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenbergwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find the words of just a selection of documents, as shown below. For more details of what information you can extract from this corpus, read the \"Gutenberg corpus\" section of the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the lectures, we can use Python's `collections.Counter` to find the most frequent words of a document from NLTK's Gutenberg collection. Below you can see how you can find the 5 most frequent words of the word list stored in the variable `emma`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "emma_counter = collections.Counter(emma)\n",
    "print(emma_counter.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "*Write Python code that prints the 10 most frequent words in each of the documents of the Gutenberg corpus. Can you identify any similarities among these list of most frequent words?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "[(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672), ('of', 4279), ('I', 3178), ('a', 3004), ('was', 2385), ('her', 2381)]\n",
      "austen-persuasion.txt\n",
      "[(',', 6750), ('the', 3120), ('to', 2775), ('.', 2741), ('and', 2739), ('of', 2564), ('a', 1529), ('in', 1346), ('was', 1330), (';', 1290)]\n",
      "austen-sense.txt\n",
      "[(',', 9397), ('to', 4063), ('.', 3975), ('the', 3861), ('of', 3565), ('and', 3350), ('her', 2436), ('a', 2043), ('I', 2004), ('in', 1904)]\n",
      "bible-kjv.txt\n",
      "[(',', 70509), ('the', 62103), (':', 43766), ('and', 38847), ('of', 34480), ('.', 26160), ('to', 13396), ('And', 12846), ('that', 12576), ('in', 12331)]\n",
      "blake-poems.txt\n",
      "[(',', 680), ('the', 351), ('.', 201), ('And', 176), ('and', 169), ('of', 131), ('I', 130), ('in', 116), ('a', 108), (\"'\", 104)]\n",
      "bryant-stories.txt\n",
      "[(',', 3481), ('the', 3086), ('and', 1873), ('.', 1817), ('to', 1165), ('a', 988), ('\"', 900), ('he', 872), ('of', 801), ('was', 706)]\n",
      "burgess-busterbrown.txt\n",
      "[('.', 823), (',', 822), ('the', 639), ('he', 562), ('and', 484), ('to', 426), (\"'\", 401), ('of', 326), ('that', 285), ('a', 275)]\n",
      "carroll-alice.txt\n",
      "[(',', 1993), (\"'\", 1731), ('the', 1527), ('and', 802), ('.', 764), ('to', 725), ('a', 615), ('I', 543), ('it', 527), ('she', 509)]\n",
      "chesterton-ball.txt\n",
      "[(',', 4547), ('the', 4523), ('.', 3589), ('of', 2529), ('and', 2488), ('a', 2184), ('\"', 1751), ('to', 1558), ('in', 1355), ('that', 1120)]\n",
      "chesterton-brown.txt\n",
      "[('the', 4321), (',', 4069), ('.', 2784), ('of', 2087), ('and', 2074), ('a', 2074), ('\"', 1461), ('to', 1378), ('in', 1205), ('was', 1141)]\n",
      "chesterton-thursday.txt\n",
      "[(',', 3488), ('the', 3291), ('.', 2717), ('a', 1713), ('of', 1710), ('and', 1568), ('\"', 1336), ('to', 1045), ('in', 888), ('I', 885)]\n",
      "edgeworth-parents.txt\n",
      "[(',', 15219), ('the', 7149), ('.', 6945), ('to', 5150), ('and', 4769), ('\"', 3880), ('of', 3730), ('I', 3656), (\"'\", 3293), ('a', 3017)]\n",
      "melville-moby_dick.txt\n",
      "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n",
      "milton-paradise.txt\n",
      "[(',', 10198), ('and', 2799), ('the', 2505), (';', 2317), ('to', 1758), ('of', 1486), ('.', 1254), ('in', 1083), ('his', 986), ('with', 876)]\n",
      "shakespeare-caesar.txt\n",
      "[(',', 2204), ('.', 1296), ('I', 531), ('the', 502), (':', 499), ('and', 409), (\"'\", 384), ('to', 370), ('you', 342), ('of', 336)]\n",
      "shakespeare-hamlet.txt\n",
      "[(',', 2892), ('.', 1886), ('the', 860), (\"'\", 729), ('and', 606), ('of', 576), ('to', 576), (':', 565), ('I', 553), ('you', 479)]\n",
      "shakespeare-macbeth.txt\n",
      "[(',', 1962), ('.', 1235), (\"'\", 637), ('the', 531), (':', 477), ('and', 376), ('I', 333), ('of', 315), ('to', 311), ('?', 241)]\n",
      "whitman-leaves.txt\n",
      "[(',', 17713), ('the', 8814), ('and', 4797), ('of', 4127), ('I', 2932), (\"'\", 2362), ('to', 1930), ('-', 1774), ('.', 1769), ('in', 1714)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "*Find the unique words with length of more than 17 characters in the complete Gutenberg corpus.*\n",
    "\n",
    "*Hint: to find the distinct items of a Python list you can convert it into a set:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c', 'b', 'a'}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "my_list = ['a','b','c','a','c']\n",
    "my_set = set(my_list)\n",
    "print(my_set)\n",
    "print(len(my_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'characteristically', 'Mahershalalhashbaz', 'uninterpenetratingly'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "*Find the words that are longer than 5 characters and occur more than 2000 times in the complete Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('little', 2825), ('before', 3335), ('people', 2773), ('children', 2223), ('should', 2496), ('against', 2255), ('Israel', 2591)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "*Find the average number of words in the documents of the NLTK Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145645.16666666666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 1.5\n",
    "*Find the Gutenberg document that has the longest average word length.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with largest average word length is milton-paradise.txt with word length 4.835734572682675\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6\n",
    "*Find the 10 most frequent bigrams in the entire Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 41294),\n",
       " (('of', 'the'), 18912),\n",
       " (('in', 'the'), 9793),\n",
       " ((\"'\", 's'), 9781),\n",
       " ((';', 'and'), 7559),\n",
       " (('and', 'the'), 6432),\n",
       " (('the', 'LORD'), 5964),\n",
       " ((',', 'the'), 5957),\n",
       " ((',', 'I'), 5677),\n",
       " ((',', 'that'), 5352)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Statistics and NLTK\n",
    "\n",
    "The following exercises use a portion of the Gutenberg corpus that is stored in the corpus dataset of NLTK. [The Project Gutenberg](http://www.gutenberg.org/) is a large collection of electronic books that are out of copyright. These books are free to download for reading, or for our case, for doing a little of corpus analysis.\n",
    "\n",
    "To obtain the list of files of NLTK's Gutenberg corpus, type the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kingtao.ng@mq.edu.au\n",
    "kingtaojasonng.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\liamf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain all words in the entire Gutenberg corpus of NLTK, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenbergwords = nltk.corpus.gutenberg.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find the total number of words, and the first 10 words (do not attempt to display all the words or your computer will freeze!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(gutenbergwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gutenbergwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find the words of just a selection of documents, as shown below. For more details of what information you can extract from this corpus, read the \"Gutenberg corpus\" section of the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emma[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the lectures, we can use Python's `collections.Counter` to find the most frequent words of a document from NLTK's Gutenberg collection. Below you can see how you can find the 5 most frequent words of the word list stored in the variable `emma`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "emma_counter = collections.Counter(emma)\n",
    "print(emma_counter.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "*Write Python code that prints the 10 most frequent words in each of the documents of the Gutenberg corpus. Can you identify any similarities among these list of most frequent words?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "[(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672), ('of', 4279), ('I', 3178), ('a', 3004), ('was', 2385), ('her', 2381)]\n",
      "austen-persuasion.txt\n",
      "[(',', 6750), ('the', 3120), ('to', 2775), ('.', 2741), ('and', 2739), ('of', 2564), ('a', 1529), ('in', 1346), ('was', 1330), (';', 1290)]\n",
      "austen-sense.txt\n",
      "[(',', 9397), ('to', 4063), ('.', 3975), ('the', 3861), ('of', 3565), ('and', 3350), ('her', 2436), ('a', 2043), ('I', 2004), ('in', 1904)]\n",
      "bible-kjv.txt\n",
      "[(',', 70509), ('the', 62103), (':', 43766), ('and', 38847), ('of', 34480), ('.', 26160), ('to', 13396), ('And', 12846), ('that', 12576), ('in', 12331)]\n",
      "blake-poems.txt\n",
      "[(',', 680), ('the', 351), ('.', 201), ('And', 176), ('and', 169), ('of', 131), ('I', 130), ('in', 116), ('a', 108), (\"'\", 104)]\n",
      "bryant-stories.txt\n",
      "[(',', 3481), ('the', 3086), ('and', 1873), ('.', 1817), ('to', 1165), ('a', 988), ('\"', 900), ('he', 872), ('of', 801), ('was', 706)]\n",
      "burgess-busterbrown.txt\n",
      "[('.', 823), (',', 822), ('the', 639), ('he', 562), ('and', 484), ('to', 426), (\"'\", 401), ('of', 326), ('that', 285), ('a', 275)]\n",
      "carroll-alice.txt\n",
      "[(',', 1993), (\"'\", 1731), ('the', 1527), ('and', 802), ('.', 764), ('to', 725), ('a', 615), ('I', 543), ('it', 527), ('she', 509)]\n",
      "chesterton-ball.txt\n",
      "[(',', 4547), ('the', 4523), ('.', 3589), ('of', 2529), ('and', 2488), ('a', 2184), ('\"', 1751), ('to', 1558), ('in', 1355), ('that', 1120)]\n",
      "chesterton-brown.txt\n",
      "[('the', 4321), (',', 4069), ('.', 2784), ('of', 2087), ('and', 2074), ('a', 2074), ('\"', 1461), ('to', 1378), ('in', 1205), ('was', 1141)]\n",
      "chesterton-thursday.txt\n",
      "[(',', 3488), ('the', 3291), ('.', 2717), ('a', 1713), ('of', 1710), ('and', 1568), ('\"', 1336), ('to', 1045), ('in', 888), ('I', 885)]\n",
      "edgeworth-parents.txt\n",
      "[(',', 15219), ('the', 7149), ('.', 6945), ('to', 5150), ('and', 4769), ('\"', 3880), ('of', 3730), ('I', 3656), (\"'\", 3293), ('a', 3017)]\n",
      "melville-moby_dick.txt\n",
      "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n",
      "milton-paradise.txt\n",
      "[(',', 10198), ('and', 2799), ('the', 2505), (';', 2317), ('to', 1758), ('of', 1486), ('.', 1254), ('in', 1083), ('his', 986), ('with', 876)]\n",
      "shakespeare-caesar.txt\n",
      "[(',', 2204), ('.', 1296), ('I', 531), ('the', 502), (':', 499), ('and', 409), (\"'\", 384), ('to', 370), ('you', 342), ('of', 336)]\n",
      "shakespeare-hamlet.txt\n",
      "[(',', 2892), ('.', 1886), ('the', 860), (\"'\", 729), ('and', 606), ('of', 576), ('to', 576), (':', 565), ('I', 553), ('you', 479)]\n",
      "shakespeare-macbeth.txt\n",
      "[(',', 1962), ('.', 1235), (\"'\", 637), ('the', 531), (':', 477), ('and', 376), ('I', 333), ('of', 315), ('to', 311), ('?', 241)]\n",
      "whitman-leaves.txt\n",
      "[(',', 17713), ('the', 8814), ('and', 4797), ('of', 4127), ('I', 2932), (\"'\", 2362), ('to', 1930), ('-', 1774), ('.', 1769), ('in', 1714)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "*Find the unique words with length of more than 17 characters in the complete Gutenberg corpus.*\n",
    "\n",
    "*Hint: to find the distinct items of a Python list you can convert it into a set:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b', 'a', 'c'}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "my_list = ['a','b','c','a','c']\n",
    "my_set = set(my_list)\n",
    "print(my_set)\n",
    "print(len(my_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "for i in nltk.corpus.gutenberg.fileids():\n",
    "    j = nltk.corpus.gutenberg.words(i);\n",
    "    for k in j: \n",
    "        if(len(k) >= 17):\n",
    "            t.append(k)\n",
    "a = []\n",
    "k = collections.Counter(t)\n",
    "for key, value in dict(k).items():\n",
    "    if(value > 2000):\n",
    "        a.append(key, value)\n",
    "\n",
    "print (a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "*Find the words that are longer than 5 characters and occur more than 2000 times in the complete Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for i in nltk.corpus.gutenberg.fileids():\n",
    "    k = nltk.corpus.gutenberg.words(i);\n",
    "    for j in k: \n",
    "        if(len(j) > 5):\n",
    "            t.append(j)\n",
    "print(set(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "*Find the average number of words in the documents of the NLTK Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-26-5311478ec952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      6\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maverage\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned)\u001b[0m\n",
      "\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    379\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[0mscl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    382\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n",
      "\u001b[0;32m    176\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 178\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    180\u001b[0m         ret = um.true_divide(\n",
      "\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "t = []\n",
    "for i in nltk.corpus.gutenberg.fileids():\n",
    "    j = nltk.corpus.gutenberg.words(i);\n",
    "    t.append(len(j))\n",
    "\n",
    "print(np.average(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 1.5\n",
    "*Find the Gutenberg document that has the longest average word length.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "milton-paradise.txt 4.835734572682675\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "for i in nltk.corpus.gutenberg.fileids():\n",
    "    n_c = len(nltk.corpus.gutenberg.raw(i))\n",
    "    n_w = len(nltk.corpus.gutenberg.words(i))\n",
    "    d[i] = n_c/n_w\n",
    "\n",
    "n = 0\n",
    "k = None\n",
    "for key, value in d.items():\n",
    "    if n < value:\n",
    "        n = value\n",
    "        k = key\n",
    "print( k , str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6\n",
    "*Find the 10 most frequent bigrams in the entire Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',', 'and'), 41294), (('of', 'the'), 18912), (('in', 'the'), 9793), ((\"'\", 's'), 9781), ((';', 'and'), 7559), (('and', 'the'), 6432), (('the', 'LORD'), 5964), ((',', 'the'), 5957), ((',', 'I'), 5677), ((',', 'that'), 5352)]\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "for i in nltk.corpus.gutenberg.fileids():\n",
    "  j = nltk.corpus.gutenberg.words(i)\n",
    "  b = list(nltk.bigrams(j))\n",
    "  t += b\n",
    "c = collections.Counter(t)\n",
    "print(c.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7\n",
    "*Find the most frequent bigram that begins with \"Moby\" in Herman Melville's \"Moby Dick\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('[', 'Moby'), 1)]\n"
     ]
    }
   ],
   "source": [
    "j = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "b = list(nltk.bigrams(j))\n",
    "\n",
    "d = [b[[w for [n,w] in b] == 'Moby']]\n",
    "c = collections.Counter(d)\n",
    "print(c.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing with NLTK\n",
    "The following exercises will ask questions about tokens, stems, and parts of speech.\n",
    "\n",
    "### Exercise 2.1\n",
    "*What is the sentence with the largest number of tokens in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While he lived, it must be only an engagement;\n",
      "but she flattered herself, that if divested of the danger of\n",
      "drawing her away, it might become an increase of comfort to him.--\n",
      "How to do her best by Harriet, was of more difficult decision;--\n",
      "how to spare her from any unnecessary pain; how to make\n",
      "her any possible atonement; how to appear least her enemy?--\n",
      "On these subjects, her perplexity and distress were very great--\n",
      "and her mind had to pass again and again through every bitter\n",
      "reproach and sorrowful regret that had ever surrounded it.--\n",
      "She could only resolve at last, that she would still avoid a\n",
      "meeting with her, and communicate all that need be told by letter;\n",
      "that it would be inexpressibly desirable to have her removed just\n",
      "now for a time from Highbury, and--indulging in one scheme more--\n",
      "nearly resolve, that it might be practicable to get an invitation\n",
      "for her to Brunswick Square.--Isabella had been pleased with Harriet;\n",
      "and a few weeks spent in London must give her some amusement.--\n",
      "She did not think it in Harriet's nature to escape being benefited\n",
      "by novelty and variety, by the streets, the shops, and the children.--\n",
      "At any rate, it would be a proof of attention and kindness in herself,\n",
      "from whom every thing was due; a separation for the present; an averting\n",
      "of the evil day, when they must all be together again.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "i = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "c = 0\n",
    "s = None\n",
    "t = 0\n",
    "for j in sent_tokenize(i):\n",
    "    if c < len(j):\n",
    "        c  = len(j)\n",
    "        s = j\n",
    "        t = len(nltk.word_tokenize(j))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "*What are the 5 most frequent parts of speech in Austen's \"Emma\"? Use the universal tag set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\liamf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Error loading universal_target: Package 'universal_target'\n",
      "[nltk_data]     not found in index\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93muniversal_tagset\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('universal_tagset')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/universal_tagset/en-ptb.map\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\liamf/nltk_data'\n    - 'C:\\\\Users\\\\liamf\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\liamf\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\liamf\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\liamf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-44-b58e63f13cb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'austen-emma.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'universal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n",
      "\u001b[0;32m    163\u001b[0m     \"\"\"\n",
      "\u001b[0;32m    164\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 165\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n",
      "\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Maps to the specified tagset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"eng\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 125\u001b[1;33m                 tagged_tokens = [\n",
      "\u001b[0m\u001b[0;32m    126\u001b[0m                     \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en-ptb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    127\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m    124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"eng\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    125\u001b[0m                 tagged_tokens = [\n",
      "\u001b[1;32m--> 126\u001b[1;33m                     \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en-ptb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    127\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    128\u001b[0m                 ]\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\mapping.py\u001b[0m in \u001b[0;36mmap_tag\u001b[1;34m(source, target, source_tag)\u001b[0m\n",
      "\u001b[0;32m    134\u001b[0m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"en-brown\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagset_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource_tag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\mapping.py\u001b[0m in \u001b[0;36mtagset_mapping\u001b[1;34m(source, target)\u001b[0m\n",
      "\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_MAPPINGS\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_MAPPINGS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"universal\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 91\u001b[1;33m             \u001b[0m_load_universal_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[1;31m# Added the new Russian National Corpus mappings because the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     93\u001b[0m             \u001b[1;31m# Russian model for nltk.pos_tag() uses it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\mapping.py\u001b[0m in \u001b[0;36m_load_universal_map\u001b[1;34m(fileid)\u001b[0m\n",
      "\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_load_universal_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_UNIVERSAL_DATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".map\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     61\u001b[0m     \u001b[1;31m# When mapping to the Universal Tagset,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n",
      "\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n",
      "\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    874\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 875\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    876\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    877\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n",
      "\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mLookupError\u001b[0m: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93muniversal_tagset\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('universal_tagset')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/universal_tagset/en-ptb.map\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\liamf/nltk_data'\n",
      "    - 'C:\\\\Users\\\\liamf\\\\anaconda3\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\liamf\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\liamf\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\liamf\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "    - ''\n",
      "**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_target')\n",
    "\n",
    "t = []\n",
    "for s in nltk.sent_tokenize(nltk.corpus.gutenberg.raw('austen-emma.txt')):\n",
    "    i = nltk.word_tokenize(s)\n",
    "    t += nltk.pos_tag(i, tagset='universal')\n",
    "\n",
    "d = {}\n",
    "\n",
    "for i in t:\n",
    "    if i[1] in d:\n",
    "        d[[i][1]] += 1\n",
    "    else:\n",
    "        d[[i][1]] = 1\n",
    "\n",
    "counter = collections.Counter(d)\n",
    "counter.most_common(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "*What is the number of distinct stems in Austen's \"Emma\"?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4731\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "t = []\n",
    "i = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "for w in i:\n",
    "    t.append(ps.stem(w))\n",
    "\n",
    "print(str(len(list(set(t)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 2.4\n",
    "*What is the most ambiguous stem in Austen's \"Emma\"? (meaning, which stem in Austen's \"Emma\" is realised in the largest number of distinct tokens?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most ambiguous stem is 'respect' with words {'Respect', 'respecting', 'respects', 'respectful', 'respectable', 'respective', 'respectfully', 'respectably', 'respected', 'respectability', 'respect'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "d = {}\n",
    "i = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "\n",
    "for w in i:\n",
    "    if ps.stem(w) in d:\n",
    "        d[ps.stem[w]] = list(set[d[ps.stem[w] + [w]])\n",
    "    else\n",
    "        d[ps.stem[w]] = [w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7\n",
    "*Find the most frequent bigram that begins with \"Moby\" in Herman Melville's \"Moby Dick\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Moby', 'Dick'), 83), (('Moby', '-'), 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing with NLTK\n",
    "The following exercises will ask questions about tokens, stems, and parts of speech.\n",
    "\n",
    "### Exercise 2.1\n",
    "*What is the sentence with the largest number of tokens in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/diego/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "The longest sentence is\n",
      "While he lived, it must be only an engagement;\n",
      "but she flattered herself, that if divested of the danger of\n",
      "drawing her away, it might become an increase of comfort to him.--\n",
      "How to do her best by Harriet, was of more difficult decision;--\n",
      "how to spare her from any unnecessary pain; how to make\n",
      "her any possible atonement; how to appear least her enemy?--\n",
      "On these subjects, her perplexity and distress were very great--\n",
      "and her mind had to pass again and again through every bitter\n",
      "reproach and sorrowful regret that had ever surrounded it.--\n",
      "She could only resolve at last, that she would still avoid a\n",
      "meeting with her, and communicate all that need be told by letter;\n",
      "that it would be inexpressibly desirable to have her removed just\n",
      "now for a time from Highbury, and--indulging in one scheme more--\n",
      "nearly resolve, that it might be practicable to get an invitation\n",
      "for her to Brunswick Square.--Isabella had been pleased with Harriet;\n",
      "and a few weeks spent in London must give her some amusement.--\n",
      "She did not think it in Harriet's nature to escape being benefited\n",
      "by novelty and variety, by the streets, the shops, and the children.--\n",
      "At any rate, it would be a proof of attention and kindness in herself,\n",
      "from whom every thing was due; a separation for the present; an averting\n",
      "of the evil day, when they must all be together again.\n",
      "The mumber of words of the longest sentence is 275\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "*What are the 5 most frequent parts of speech in Austen's \"Emma\"? Use the universal tag set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VERB', 35723),\n",
       " ('NOUN', 31998),\n",
       " ('.', 30304),\n",
       " ('PRON', 21263),\n",
       " ('ADP', 17880)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "*What is the number of distinct stems in Austen's \"Emma\"?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austen's Emma has 5394 distinct stems\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 2.4\n",
    "*What is the most ambiguous stem in Austen's \"Emma\"? (meaning, which stem in Austen's \"Emma\" is realised in the largest number of distinct tokens?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most ambiguous stem is 'respect' with words {'Respect', 'respecting', 'respects', 'respectful', 'respectable', 'respective', 'respectfully', 'respectably', 'respected', 'respectability', 'respect'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('comp3220': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
